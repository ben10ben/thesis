{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succesfull\n"
     ]
    }
   ],
   "source": [
    "# TODO which one?\n",
    "#git clone https://github.com/lucidrains/iTransformer.git\n",
    "#import iTransformer\n",
    "import sys\n",
    "sys.path.append('/vol/fob-vol7/nebenf21/reinbene/bene/MA/iTransformer') \n",
    "from iTransformer import iTransformer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "window_size = 96\n",
    "pred_length = (96)\n",
    "\n",
    "from utils import data_handling, training_functions, helpers\n",
    "import config \n",
    "\n",
    "print(\"Import succesfull\")\n",
    "\n",
    "# use full train dataset for training or small 4% subset\n",
    "four_weeks = -24*7*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark all datasets used for transfer learning on iTransformer on its own.\n",
    "\n",
    "After the sanity check, we will use 96h as the input and the prediction horizon. In the following we will use the RevIn normalization strategie to benchmark iTransformer on our transfer learning datasets to get an initial baseline. RevIn is used because if performed the best on our initial baseline. This baseline will be compared to an SARIMA implementation and an additional DL model.\n",
    "\n",
    "Those results will also be used to evaluate the transfer-learning capability of iTransformer.\n",
    "\n",
    "Because transfer-learning is a sensible solution for the cold-start problem, we also do benchmarks on iTransformer only trained on the first 10% of the train dataset. Beacuse all datasets are big enough for efficient training, using a small subset is a solutoin to get meaningfull insight in how much value can be created through transfer-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 348])\n"
     ]
    }
   ],
   "source": [
    "# use electricity dataset\n",
    "data_dict = data_handling.load_electricity()\n",
    "\n",
    "electricity = {}\n",
    "electricity[\"dataloader_train\"], electricity[\"dataloader_validation\"], electricity[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "data_dict[\"train\"].shape\n",
    "\n",
    "# create a smaller subset of the train dataset\n",
    "electricity[\"4_weeks_train\"] = data_dict[\"train\"][four_weeks:,:]\n",
    "electricity[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(electricity[\"4_weeks_train\"] , window_size, pred_length)\n",
    "electricity[\"4_weeks_train\"] = data_handling.DataLoader(electricity[\"4_weeks_train\"] , batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 1454])\n"
     ]
    }
   ],
   "source": [
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to dataloader\n",
    "genome_project = {}\n",
    "genome_project[\"dataloader_train\"], genome_project[\"dataloader_validation\"], genome_project[\"dataloader_test\"] = data_handling.convert_data(gp_dict, window_size, pred_length)\n",
    "\n",
    "# create a smaller subset of the train dataset\n",
    "genome_project[\"4_weeks_train\"] = gp_dict[\"train\"][four_weeks:,:]\n",
    "genome_project[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(genome_project[\"4_weeks_train\"] , window_size, pred_length)\n",
    "genome_project[\"4_weeks_train\"] = data_handling.DataLoader(genome_project[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 59])\n"
     ]
    }
   ],
   "source": [
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "bavaria = {}\n",
    "bavaria[\"dataloader_train\"], bavaria[\"dataloader_validation\"], bavaria[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# create a smaller subset of the train dataset\n",
    "bavaria[\"4_weeks_train\"] = data_dict[\"train\"][four_weeks:,:]\n",
    "bavaria[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(bavaria[\"4_weeks_train\"] , window_size, pred_length)\n",
    "bavaria[\"4_weeks_train\"] = data_handling.DataLoader(bavaria[\"4_weeks_train\"] , batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiment for each dataset and save model and evaluation metrics\n",
    "dataset_dict = {\n",
    "              #  \"electricity\": electricity,\n",
    "             #   \"bavaria\": bavaria,\n",
    "                \"genome_project\" : genome_project,                \n",
    "                }\n",
    "\n",
    "\n",
    "def train_and_evaluate(dataset_dict, dataset_name, full_dataset= True, epoch=20):\n",
    "\n",
    "    if full_dataset == False:\n",
    "        print(\"Selecting 4 week dataset\")\n",
    "        training_dataloader = dataset_dict[\"4_weeks_train\"]\n",
    "    else:\n",
    "        training_dataloader = dataset_dict[\"dataloader_train\"]\n",
    "        \n",
    "    inputs, _ = next(iter(training_dataloader))\n",
    "    num_variates = inputs.size(2)\n",
    "    \n",
    "    # define parameters and create config \n",
    "    best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.2, \n",
    "                    'num_mem_tokens': 4, 'learning_rate': 0.0005}\n",
    "\n",
    "    best_parameters = {'depth': 2, 'dim': 512, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.2, \n",
    "                    'num_mem_tokens': 4, 'learning_rate': 0.0005}\n",
    "\n",
    "    model_config = {\n",
    "        'num_variates': num_variates,\n",
    "        'lookback_len': window_size,\n",
    "        'depth': best_parameters[\"depth\"],\n",
    "        'dim': best_parameters[\"dim\"],\n",
    "        'num_tokens_per_variate': 1,\n",
    "        'pred_length': pred_length,\n",
    "        'dim_head': best_parameters[\"dim_head\"],\n",
    "        'heads': best_parameters[\"heads\"],\n",
    "        'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "        'ff_mult': best_parameters[\"ff_mult\"],\n",
    "        'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "        'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "        'use_reversible_instance_norm': True,\n",
    "        'reversible_instance_norm_affine': True,\n",
    "        'flash_attn': True\n",
    "    }\n",
    "\n",
    "    # select available deviec\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # defining all needed instances\n",
    "    model = iTransformer(**model_config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_parameters[\"learning_rate\"])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[\"iTransformer_baseline\"] / dataset_name)\n",
    "\n",
    "    # run model training as mentioned in the original paper\n",
    "    if full_dataset == False:\n",
    "        checkpoint_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / dataset_name / f\"{dataset_name}_4_weeks_best_val_loss.pt\"\n",
    "    else:\n",
    "        checkpoint_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / dataset_name / f\"{dataset_name}_full_dataset_best_val_loss.pt\"\n",
    "\n",
    "\n",
    "    # load model with best validaiton mse\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model = iTransformer(**model_config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        epoch = epoch - checkpoint[\"epoch\"]\n",
    "        if epoch == 0:\n",
    "            print(\"Model is already trained for 20 epochs.\")\n",
    "            return None\n",
    "    except:\n",
    "        print(\"Training from scratch.\")\n",
    "\n",
    "    train_metrics, best_model = training_functions.train_one_epoch(epoch, model, device, training_dataloader, dataset_dict[\"dataloader_validation\"], \\\n",
    "                                            optimizer, scheduler, writer, checkpoint_path)\n",
    "\n",
    "\n",
    "    # predict on test set\n",
    "    metrics = helpers.full_eval(best_model, dataset_dict[\"dataloader_test\"], device)\n",
    "    for eval_metric, value in metrics[96].items():\n",
    "        metrics[96][eval_metric] = value.item()\n",
    "\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "    metrics_df.rename(columns={0: dataset_name}, inplace=True)\n",
    "\n",
    "    if full_dataset == False:\n",
    "        metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{dataset_name}_epochs{epoch}_4_week_dataset.csv\")\n",
    "    else:\n",
    "        metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{dataset_name}_epochs_{epoch}baseline.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in dataset_dict.items():\n",
    "#    train_and_evaluate(value, key, full_dataset=False, epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genome_project\n",
      "Using device: cuda\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 314/314 [03:50<00:00,  1.36it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n",
      "Checkpointing succesfull after epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 314/314 [06:06<00:00,  1.17s/it]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:43<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 314/314 [06:54<00:00,  1.32s/it]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:41<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 314/314 [03:12<00:00,  1.63it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 314/314 [03:03<00:00,  1.71it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:11<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 314/314 [03:21<00:00,  1.56it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:13<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 314/314 [03:06<00:00,  1.69it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:11<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 314/314 [03:11<00:00,  1.64it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:12<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 314/314 [02:58<00:00,  1.76it/s]\n",
      "Epoch: Validating: 100%|██████████| 40/40 [00:12<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(1.5894, device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:22<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, value in dataset_dict.items():\n",
    "    print(key)\n",
    "    train_and_evaluate(value, key, full_dataset=True, epoch=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
