{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succesfull\n"
     ]
    }
   ],
   "source": [
    "# TODO which one?\n",
    "#git clone https://github.com/lucidrains/iTransformer.git\n",
    "#import iTransformer\n",
    "import sys\n",
    "sys.path.append('/vol/fob-vol7/nebenf21/reinbene/bene/MA/iTransformer') \n",
    "from iTransformer import iTransformer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "window_size = 96\n",
    "pred_length = (96)\n",
    "\n",
    "from utils import data_handling, training_functions, helpers\n",
    "import config \n",
    "\n",
    "print(\"Import succesfull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark all datasets used for transfer learning on iTransformer on its own.\n",
    "\n",
    "After the sanity check, we will use 96h as the input and the prediction horizon. In the following we will use the RevIn normalization strategie to benchmark iTransformer on our trnasfer learning datasets to get an initial baseline. This baseline will be compared to an SARIMA implementation and an additional DL model.\n",
    "\n",
    "Those results will also be used to evaluate the transfer-learning capability of iTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 348])\n"
     ]
    }
   ],
   "source": [
    "# use electricity dataset\n",
    "data_dict = data_handling.load_electricity()\n",
    "\n",
    "electricity = {}\n",
    "electricity[\"dataloader_train\"], electricity[\"dataloader_validation\"], electricity[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 208])\n"
     ]
    }
   ],
   "source": [
    "# eu electricity data\n",
    "data_tensor = data_handling.eu_electricity_to_tensor()\n",
    "data_dict, standardize_values =  data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to dataloader\n",
    "europe = {}\n",
    "europe[\"dataloader_train\"], europe[\"dataloader_validation\"], europe[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 67])\n"
     ]
    }
   ],
   "source": [
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "bavaria = {}\n",
    "bavaria[\"dataloader_train\"], bavaria[\"dataloader_validation\"], bavaria[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"electricity\" : electricity,\n",
    "\t\t\t\"europe\" : europe,\n",
    "\t\t\t\"bavaria\" : bavaria\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"electricity\" : electricity}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 151/151 [02:14<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE-Loss: 0.0669880151896682, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:17<00:00,  1.19it/s]\n",
      "Epoch: 2: 100%|██████████| 151/151 [02:25<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, MSE-Loss: 0.04674160890034493, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.06it/s]\n",
      "Epoch: 3: 100%|██████████| 151/151 [02:27<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, MSE-Loss: 0.041897441402373725, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.08it/s]\n",
      "Epoch: 4: 100%|██████████| 151/151 [02:32<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, MSE-Loss: 0.039079121517523234, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.07it/s]\n",
      "Epoch: 5: 100%|██████████| 151/151 [02:28<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, MSE-Loss: 0.037844702502749614, LR: 0.0005\n",
      "Checkpointing succesfull after epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.10it/s]\n",
      "Epoch: 6: 100%|██████████| 151/151 [02:26<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, MSE-Loss: 0.036871165974645424, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:18<00:00,  1.14it/s]\n",
      "Epoch: 7: 100%|██████████| 151/151 [02:23<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, MSE-Loss: 0.035841628576055266, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.07it/s]\n",
      "Epoch: 8: 100%|██████████| 151/151 [02:18<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, MSE-Loss: 0.03508379032823029, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:12<00:00,  1.68it/s]\n",
      "Epoch: 9: 100%|██████████| 151/151 [02:29<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, MSE-Loss: 0.03479766068573029, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:19<00:00,  1.07it/s]\n",
      "Epoch: 10: 100%|██████████| 151/151 [01:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, MSE-Loss: 0.03430093189590419, LR: 0.0005\n",
      "Checkpointing succesfull after epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 21/21 [00:00<00:00, 59.06it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "full_eval() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     45\u001b[0m     training_functions\u001b[38;5;241m.\u001b[39mtrain_one_epoch(epoch, model, device, dataset_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_train\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m], optimizer, scheduler, writer)\n\u001b[0;32m---> 48\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataloader_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_metric, value \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;241m96\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     51\u001b[0m     metrics[\u001b[38;5;241m96\u001b[39m][eval_metric] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: full_eval() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "# run experiment for each normalizaiton strategie and save model and evaluation metrics\n",
    "\n",
    "for key, dataset_dict in datasets.items():\n",
    "    torch.cuda.empty_cache()\n",
    "    inputs, _ = next(iter(dataset_dict[\"dataloader_train\"]))\n",
    "    num_variates = inputs.size(2)\n",
    "    \n",
    "    # define parameters and create config \n",
    "    best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.1, \n",
    "                    'num_mem_tokens': 4, 'learning_rate': 0.0005}\n",
    "\n",
    "\n",
    "    model_config = {\n",
    "        'num_variates': num_variates,\n",
    "        'lookback_len': window_size,\n",
    "        'depth': best_parameters[\"depth\"],\n",
    "        'dim': best_parameters[\"dim\"],\n",
    "        'num_tokens_per_variate': 1,\n",
    "        'pred_length': pred_length,\n",
    "        'dim_head': best_parameters[\"dim_head\"],\n",
    "        'heads': best_parameters[\"heads\"],\n",
    "        'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "        'ff_mult': best_parameters[\"ff_mult\"],\n",
    "        'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "        'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "        'use_reversible_instance_norm': True,\n",
    "        'reversible_instance_norm_affine': True,\n",
    "        'flash_attn': True\n",
    "    }\n",
    "\n",
    "    # select available deviec\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # defining all needed instances\n",
    "    model = iTransformer(**model_config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_parameters[\"learning_rate\"])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[\"iTransformer_baseline\"])\n",
    "\n",
    "    # run model training as mentioned in the original paper\n",
    "    epoch = 10\n",
    "\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        training_functions.train_one_epoch(epoch, model, device, dataset_dict[\"dataloader_train\"], dataset_dict[\"dataloader_validation\"], optimizer, scheduler, writer)\n",
    "\n",
    "\n",
    "    metrics = helpers.full_eval(model, dataset_dict[\"dataloader_test\"], device)\n",
    "\n",
    "    for eval_metric, value in metrics[96].items():\n",
    "        metrics[96][eval_metric] = value.item()\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "    metrics_df.rename(columns={0: key}, inplace=True)\n",
    "\n",
    "    metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{key}_epochs{epoch}_revin.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
