{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as Arima\n",
    "from utils import data_handling, helpers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import config\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do long and short predictions using ARIMA\n",
    "\n",
    "For every ID in all three datasets we fit an ARIMA model and do predictions over every 96 time step window of each datasets test set.\n",
    "\n",
    "We take the last 2000 datapoints of the train set for \"full\" model training for computational reasons. \n",
    "For the jumpstart evaluation we also train all ARIMA models on around 350 time steps which equals the 2 week fine-tuning horizon used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set: 209 days, 0:00:00\n",
      "Length validation set: 34 days, 0:00:00\n",
      "Saving train, validation and test df for faster loading\n"
     ]
    }
   ],
   "source": [
    "# use electricity dataset\n",
    "electricity_dict = data_handling.format_electricity()\n",
    "\n",
    "\n",
    "for key, value in electricity_dict.items():\n",
    "\t\t\telectricity_dict[key]= data_handling.df_to_tensor(value)\n",
    "train_standardize_dict = None\n",
    "\n",
    "# normalize train and use matrics for val and test\n",
    "electricity_dict[\"train\"], train_standardize_dict = helpers.custom_standardizer(electricity_dict[\"train\"])\n",
    "electricity_dict[\"validation\"], _ = helpers.custom_standardizer(electricity_dict[\"validation\"], train_standardize_dict)\n",
    "electricity_dict[\"test\"], _ = helpers.custom_standardizer(electricity_dict[\"test\"], train_standardize_dict)\n",
    "\n",
    "# load bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "bavaria_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_tensor(df, lag):\n",
    "    if lag > 0:\n",
    "        return torch.cat((torch.zeros(lag, df.size(1)), df[:-lag]), dim=0)\n",
    "    return df\n",
    "\n",
    "# Example tensor of shape [2929, 348]\n",
    "def create_lagged(df):\n",
    "\n",
    "    # Lag by 24, ...\n",
    "    lagged_1 = lag_tensor(df, 1)\n",
    "    lagged_24 = lag_tensor(df, 24)\n",
    "    lagged_48 = lag_tensor(df, 24*2)\n",
    "    lagged_72 = lag_tensor(df, 24*3)\n",
    "    lagged_96 = lag_tensor(df, 24*4)\n",
    "\n",
    "    length = df.size(0)\n",
    "    ids = df.size(1)\n",
    "\n",
    "    # create time of day index\n",
    "    hours = torch.arange(0, 24)\n",
    "\n",
    "    # implement sin/cosine encoding for 24h\n",
    "    sin_encodings = torch.sin(2 * torch.pi * hours / 24)\n",
    "    cos_encodings = torch.cos(2 * torch.pi * hours / 24)\n",
    "\n",
    "    time_of_day_sin = sin_encodings.repeat(ids, length//23).transpose(0,1)[:length,:]\n",
    "    time_of_day_cos = cos_encodings.repeat(ids, length//23).transpose(0,1)[:length,:]\n",
    "    \n",
    "   # return torch.stack((lagged_24, lagged_48, lagged_72, lagged_96, time_of_day_sin, time_of_day_cos), dim=2)\n",
    "    return torch.stack((lagged_1, lagged_24, lagged_48, lagged_72, lagged_96, time_of_day_sin, time_of_day_cos), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(df, dataset_name, data_split_description):\n",
    "    num_96_horizons = int(df[\"test\"][:,0].shape[0] / (96))\n",
    "    lagged_covariates_train = create_lagged(df[\"train\"])\n",
    "    lagged_covariates_test = create_lagged(df[\"test\"])\n",
    "\n",
    "    #filename = config.CONFIG_OUTPUT_PATH[\"arima\"] / f'arima_{key_}predictions.csv'\n",
    "    filename = config.CONFIG_OUTPUT_PATH[\"arima\"] / f'arima_{dataset_name}_predictions{data_split_description}_new.pkl'\n",
    "\n",
    "\n",
    "    # load predictions if available\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            prediction_list = pickle.load(file)\n",
    "    except: \n",
    "        print(\"no predictions available.\")\n",
    "        prediction_list = []\n",
    "\n",
    "\n",
    "    # fit a model for each id and iterate over the test split for inference\n",
    "    for id in range(len(prediction_list), df[\"train\"].size(1)):\n",
    "        model = Arima.auto_arima(df[\"train\"][-2000:,id], exogenous=lagged_covariates_train[-2000:,id,:], stepwise=True, seasonal=True, m=24, maxiter=3)\n",
    "\n",
    "        sum_mse = 0\n",
    "        sum_mae = 0\n",
    "        sum_mape = 0\n",
    "        for i in range(num_96_horizons):\n",
    "            time_step = i * 96\n",
    "            target = df[\"test\"][time_step : time_step+96, id]\n",
    "\n",
    "            lagged_window_test = lagged_covariates_test[time_step:time_step+96,id,:]\n",
    "            forecasts = model.predict(n_periods=96, return_conf_int=False, exogenous=lagged_window_test, alpha=0.1)\n",
    "\n",
    "            # evaluate forecasts\n",
    "            sum_mse += mean_squared_error(forecasts, target)\n",
    "            sum_mae += mean_absolute_error(forecasts, target)\n",
    "            sum_mape = mean_absolute_percentage_error(forecasts, target)\n",
    "            \n",
    "\n",
    "        prediction_list.append([sum_mse / num_96_horizons, sum_mae / num_96_horizons, sum_mape / num_96_horizons])\n",
    "        print(f\"MSE of {id}: \", sum_mse / num_96_horizons, \"MAE:\" ,sum_mae / num_96_horizons)\n",
    "\n",
    "\n",
    "        # save as pickle\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(prediction_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of 89:  0.32927096978831244 MAE: 0.33346301309592197\n",
      "MSE of 90:  1.5658593170538684 MAE: 1.1696933567474412\n",
      "MSE of 91:  0.9549585397580441 MAE: 0.7757665413783404\n",
      "MSE of 92:  0.4543028745925838 MAE: 0.45597753125305984\n",
      "MSE of 93:  0.46771367291224236 MAE: 0.5586711658770847\n",
      "MSE of 94:  0.13252071945347918 MAE: 0.33551136778405766\n",
      "MSE of 95:  1.0278528239623763 MAE: 0.8196911443594631\n",
      "MSE of 96:  1.3447990959024314 MAE: 0.7697593820801617\n",
      "MSE of 97:  0.05258738903343237 MAE: 0.1879964907019307\n",
      "MSE of 98:  0.3245768039055557 MAE: 0.4707164346949835\n",
      "MSE of 99:  1.3699501966056666 MAE: 0.9559473745615649\n",
      "MSE of 100:  0.6892523581955126 MAE: 0.658790931834944\n",
      "MSE of 101:  0.38499577000274104 MAE: 0.5139291851700877\n",
      "MSE of 102:  1.2075823568634396 MAE: 0.8647181950049422\n",
      "MSE of 103:  1.2408763422465838 MAE: 0.8941636225328643\n",
      "MSE of 104:  7.984314598505195 MAE: 2.597452709413577\n",
      "MSE of 105:  1.9522577660422575 MAE: 1.12718512423622\n",
      "MSE of 106:  0.3924259181193831 MAE: 0.49598670131700173\n",
      "MSE of 107:  0.7257356199801197 MAE: 0.6278774414907894\n",
      "MSE of 108:  0.36350640998175093 MAE: 0.4827801751442396\n",
      "MSE of 109:  1.4969255229323815 MAE: 0.883052537793898\n",
      "MSE of 110:  0.52440694424908 MAE: 0.5570125441521919\n",
      "MSE of 111:  0.7154264644763294 MAE: 0.68526066094746\n",
      "MSE of 112:  0.7747190211010762 MAE: 0.7025562955421758\n",
      "MSE of 113:  0.6845286621118026 MAE: 0.6765999467197569\n",
      "MSE of 114:  0.03725355819887848 MAE: 0.153769990296287\n",
      "MSE of 115:  0.24717475126858587 MAE: 0.34931558299444876\n",
      "MSE of 116:  1.4595501475218633 MAE: 0.8621091854624507\n",
      "MSE of 117:  2.15503457388724 MAE: 1.1140962397271703\n",
      "MSE of 118:  0.6692832785669017 MAE: 0.6252063973518013\n",
      "MSE of 119:  6.4528847907746405 MAE: 2.403337769101982\n",
      "MSE of 120:  0.8241079234013357 MAE: 0.760589813764272\n",
      "MSE of 121:  3.1244276058644567 MAE: 1.6785204125289659\n",
      "MSE of 122:  1.2701940954191453 MAE: 0.8667454360951832\n",
      "MSE of 123:  2.8879109969830883 MAE: 1.3855772157518877\n"
     ]
    }
   ],
   "source": [
    "# do training on 2000 time steps\n",
    "#process_and_predict(electricity_dict, \"electricity\", \"full\")\n",
    "#process_and_predict(bavaria_dict, \"bavaria\", \"full\")\n",
    "process_and_predict(gp_dict, \"genome_project\", \"full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
