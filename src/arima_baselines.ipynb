{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as Arima\n",
    "from utils import data_handling, helpers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import config\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do long and short predictions using ARIMA\n",
    "\n",
    "For every ID in all three datasets we fit an ARIMA model and do predictions over every 96 time step window of each datasets test set.\n",
    "\n",
    "We take the last 2000 datapoints of the train set for \"full\" model training for computational reasons. \n",
    "For the jumpstart evaluation we also train all ARIMA models on around 350 time steps which equals the 2 week fine-tuning horizon used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set: 209 days, 0:00:00\n",
      "Length validation set: 34 days, 0:00:00\n",
      "Saving train, validation and test df for faster loading\n"
     ]
    }
   ],
   "source": [
    "# use electricity dataset\n",
    "electricity_dict = data_handling.format_electricity()\n",
    "\n",
    "\n",
    "for key, value in electricity_dict.items():\n",
    "\t\t\telectricity_dict[key]= data_handling.df_to_tensor(value)\n",
    "train_standardize_dict = None\n",
    "\n",
    "# normalize train and use matrics for val and test\n",
    "electricity_dict[\"train\"], train_standardize_dict = helpers.custom_standardizer(electricity_dict[\"train\"])\n",
    "electricity_dict[\"validation\"], _ = helpers.custom_standardizer(electricity_dict[\"validation\"], train_standardize_dict)\n",
    "electricity_dict[\"test\"], _ = helpers.custom_standardizer(electricity_dict[\"test\"], train_standardize_dict)\n",
    "\n",
    "# load bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "bavaria_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_tensor(df, lag):\n",
    "    if lag > 0:\n",
    "        return torch.cat((torch.zeros(lag, df.size(1)), df[:-lag]), dim=0)\n",
    "    return df\n",
    "\n",
    "# Example tensor of shape [2929, 348]\n",
    "def create_lagged(df):\n",
    "\n",
    "    # Lag by 24, ...\n",
    "    lagged_24 = lag_tensor(df, 24)\n",
    "    lagged_48 = lag_tensor(df, 24*2)\n",
    "    lagged_72 = lag_tensor(df, 24*3)\n",
    "    lagged_96 = lag_tensor(df, 24*4)\n",
    "\n",
    "    length = df.size(0)\n",
    "    ids = df.size(1)\n",
    "\n",
    "    # create time of day index\n",
    "    hours = torch.arange(0, 24)\n",
    "\n",
    "    # implement sin/cosine encoding for 24h\n",
    "    sin_encodings = torch.sin(2 * torch.pi * hours / 24)\n",
    "    cos_encodings = torch.cos(2 * torch.pi * hours / 24)\n",
    "\n",
    "    time_of_day_sin = sin_encodings.repeat(ids, length//23).transpose(0,1)[:length,:]\n",
    "    time_of_day_cos = cos_encodings.repeat(ids, length//23).transpose(0,1)[:length,:]\n",
    "    \n",
    "    return torch.stack((lagged_24, lagged_48, lagged_72, lagged_96, time_of_day_sin, time_of_day_cos), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged = create_lagged(bavaria_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(df, dataset_name, data_split_description):\n",
    "    num_96_horizons = int(df[\"test\"][:,0].shape[0] / (96))\n",
    "    lagged_covariates_train = create_lagged(df[\"train\"])\n",
    "    lagged_covariates_test = create_lagged(df[\"test\"])\n",
    "\n",
    "    #filename = config.CONFIG_OUTPUT_PATH[\"arima\"] / f'arima_{key_}predictions.csv'\n",
    "    filename = config.CONFIG_OUTPUT_PATH[\"arima\"] / f'arima_{dataset_name}_predictions{data_split_description}.pkl'\n",
    "\n",
    "\n",
    "    # Open the file and read the data\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            prediction_list = pickle.load(file)\n",
    "    except: \n",
    "        print(\"no predictions available.\")\n",
    "        prediction_list = []\n",
    "\n",
    "\n",
    "    for id in range(len(prediction_list), df[\"train\"].size(1)):\n",
    "        model = Arima.auto_arima(df[\"train\"][-2000:,id], exogenous=lagged_covariates_train[-2000:,id,:], stepwise=True, seasonal=True, m=24, maxiter=3)\n",
    "\n",
    "        sum_mse = 0\n",
    "        sum_mae = 0\n",
    "        sum_mape = 0\n",
    "        for i in range(num_96_horizons):\n",
    "            time_step = i * 96\n",
    "            target = df[\"test\"][time_step : time_step+96, id]\n",
    "\n",
    "            lagged_window_test = lagged_covariates_test[time_step:time_step+96,id,:]\n",
    "            forecasts = model.predict(n_periods=96, return_conf_int=False, exogenous=lagged_window_test, alpha=0.1)\n",
    "\n",
    "            sum_mse += mean_squared_error(forecasts, target)\n",
    "            sum_mae += mean_absolute_error(forecasts, target)\n",
    "            sum_mape = mean_absolute_percentage_error(forecasts, target)\n",
    "            \n",
    "\n",
    "        prediction_list.append([sum_mse / num_96_horizons, sum_mae / num_96_horizons, sum_mape / num_96_horizons])\n",
    "        print(f\"MSE of {id}: \", sum_mse / num_96_horizons)\n",
    "\n",
    "\n",
    "        # save as pickle\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(prediction_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of 547:  3.166729600731978\n",
      "MSE of 548:  1.1696219405663149\n",
      "MSE of 549:  1.9622510599295693\n",
      "MSE of 550:  3.291421319398892\n",
      "MSE of 551:  0.9660356550743359\n",
      "MSE of 552:  3.965891833960476\n",
      "MSE of 553:  0.23156887636406356\n",
      "MSE of 554:  0.6089991630596672\n",
      "MSE of 555:  2.4816630955018395\n",
      "MSE of 556:  0.9491114861744929\n",
      "MSE of 557:  0.69992819277287\n",
      "MSE of 558:  1.038860681801881\n"
     ]
    }
   ],
   "source": [
    "# do training on 2000 time steps\n",
    "#process_and_predict(electricity_dict, \"electricity\", \"full\")\n",
    "#process_and_predict(bavaria_dict, \"bavaria\", \"full\")\n",
    "process_and_predict(gp_dict, \"genome_project\", \"full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
