{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/lucidrains/iTransformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben_ten/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ben_ten/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "#%pip install iTransformer\n",
    "#!python3 -m pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117\n",
    "#!pip install holidays\n",
    "\n",
    "from iTransformer import iTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import holidays\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from beartype.typing import Union, Tuple\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.helpers import SlidingWindowTimeSeriesDataset, ModelConfig, create_checkpoint, calc_percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iTransformer from paper, difficult to use // dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of paper iTransformer\n",
    "\n",
    "from model.iTransformer import Model \n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "configs = SimpleNamespace(\n",
    "    seq_len=100,\n",
    "    pred_len=10,\n",
    "    output_attention=True,\n",
    "    d_model=512,\n",
    "    embed='some_embedding_config',\n",
    "    freq='H',  # e.g., hourly frequency\n",
    "    dropout=0.1,\n",
    "    class_strategy='some_strategy',\n",
    "    factor=5,\n",
    "    e_layers=6,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    activation='relu'  # e.g., relu activation\n",
    ")\n",
    "\n",
    "model = Model(configs)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "#class ModelConfig:\n",
    "\tseq_len: int = 96\n",
    "\tpred_len: int = 24\n",
    "\toutput_attention: int =  \n",
    "\td_model\n",
    "\tembed\n",
    "\tfreq\n",
    "\tdropout\n",
    "\tclass_strategy\n",
    "\tfactor\n",
    "\tn_heads\n",
    "\td_ff\n",
    "\tactivation\n",
    "\te_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198067</th>\n",
       "      <td>20824.324324</td>\n",
       "      <td>32299.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198068</th>\n",
       "      <td>19527.027027</td>\n",
       "      <td>32300.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198069</th>\n",
       "      <td>20202.702703</td>\n",
       "      <td>32301.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198070</th>\n",
       "      <td>19851.351351</td>\n",
       "      <td>32302.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198071</th>\n",
       "      <td>20135.135135</td>\n",
       "      <td>32303.0</td>\n",
       "      <td>1345</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          power_usage  time_idx  days_from_start      id  hour  day  \\\n",
       "0            2.538071   26304.0             1096  MT_001     0    1   \n",
       "1            2.855330   26305.0             1096  MT_001     1    1   \n",
       "2            2.855330   26306.0             1096  MT_001     2    1   \n",
       "3            2.855330   26307.0             1096  MT_001     3    1   \n",
       "4            2.538071   26308.0             1096  MT_001     4    1   \n",
       "...               ...       ...              ...     ...   ...  ...   \n",
       "2198067  20824.324324   32299.0             1345  MT_370    19    7   \n",
       "2198068  19527.027027   32300.0             1345  MT_370    20    7   \n",
       "2198069  20202.702703   32301.0             1345  MT_370    21    7   \n",
       "2198070  19851.351351   32302.0             1345  MT_370    22    7   \n",
       "2198071  20135.135135   32303.0             1345  MT_370    23    7   \n",
       "\n",
       "         day_of_week  month  \n",
       "0                  2      1  \n",
       "1                  2      1  \n",
       "2                  2      1  \n",
       "3                  2      1  \n",
       "4                  2      1  \n",
       "...              ...    ...  \n",
       "2198067            6      9  \n",
       "2198068            6      9  \n",
       "2198069            6      9  \n",
       "2198070            6      9  \n",
       "2198071            6      9  \n",
       "\n",
       "[2100000 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passenger_df = pd.read_csv(Path(\"/home/ben_ten/ben/MA/air_passenger/AirPassengers.csv\"), delimiter=\",\")\n",
    "\n",
    "electricity = pd.read_csv(Path(\"/home/ben_ten/ben/MA/LD2011_2014.csv\"), delimiter=\",\")\n",
    "electricity.drop(['categorical_hour', 'categorical_day_of_week', 'hours_from_start', 'Unnamed: 0', 'categorical_id', 'date'], axis=1, inplace=True)\n",
    "for i in electricity[\"id\"].unique():\n",
    "\tif electricity[electricity[\"id\"]==i][\"days_from_start\"].min() != 1096:\n",
    "\t\telectricity.drop(electricity[electricity[\"id\"] == i].index, inplace=True)\n",
    "electricity.to_csv(\"electricity_small.csv\", index=False)\n",
    "electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataprep pessenger\n",
    "\n",
    "# Sample pandas series\n",
    "passenger = passenger_df['#Passengers']\n",
    "\n",
    "# Convert the pandas series to a NumPy array\n",
    "numpy_arr = passenger.to_numpy()\n",
    "\n",
    "# Reshape the NumPy array to have shape (1, Series, 1)\n",
    "reshaped_array = numpy_arr.reshape(1, -1, 1)\n",
    "\n",
    "# Convert the reshaped NumPy array to a PyTorch tensor\n",
    "torch_tensor = torch.Tensor(reshaped_array)\n",
    "\n",
    "# Check the shape of the PyTorch tensor\n",
    "print(torch_tensor.shape)\n",
    "\n",
    "def min_max_scaling(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor\n",
    "\n",
    "def standard_scaling(tensor):\n",
    "    mean = tensor.mean(dim=(0, 1), keepdim=True)\n",
    "    std = tensor.std(dim=(0, 1), unbiased=False, keepdim=True)  # using unbiased=False for sample standard deviation\n",
    "    scaled_tensor = (tensor - mean) / (std + 1e-7) \n",
    "    return scaled_tensor\n",
    "\n",
    "#uncommend for normalized/standardized tensors\n",
    "#torch_tensor = min_max_scaling(torch_tensor)\n",
    "torch_tensor = standard_scaling(torch_tensor)\n",
    "\n",
    "tensor_x = torch_tensor[:, :132, :]\n",
    "tensor_y = torch_tensor[:, 132:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for electricity and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20301/201167750.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[variates] = electricity_scaler.transform(train[variates])\n",
      "/tmp/ipykernel_20301/201167750.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[variates] = electricity_scaler.transform(test[variates])\n",
      "/tmp/ipykernel_20301/201167750.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid[variates] = electricity_scaler.transform(valid[variates])\n",
      "/tmp/ipykernel_20301/201167750.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.sort_values(by=['id', 'time_idx'], inplace=True)\n",
      "/tmp/ipykernel_20301/201167750.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.sort_values(by=['id', 'time_idx'], inplace=True)\n",
      "/tmp/ipykernel_20301/201167750.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.sort_values(by=['id', 'time_idx'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# execute this to fully prep 1 batch electricity for iTrasnfromer \n",
    "\n",
    "def split_data(df, valid_boundary=1315, test_boundary=1339, use_scaler=True):\n",
    "\tindex = df['days_from_start']\n",
    "\ttrain = df.loc[index < valid_boundary]\n",
    "\tvalid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "\ttest = df.loc[index >= test_boundary - 7]\n",
    "\n",
    "\tif use_scaler == True:\n",
    "\t\telectricity_scaler = StandardScaler()\n",
    "\t\n",
    "\t\tvariates =  ['power_usage', 'time_idx', 'days_from_start', 'hour', 'day', 'day_of_week', 'month']\n",
    "\t\telectricity_scaler.fit(train[variates])\n",
    "\t\ttrain[variates] = electricity_scaler.transform(train[variates])\n",
    "\t\ttest[variates] = electricity_scaler.transform(test[variates])\n",
    "\t\tvalid[variates] = electricity_scaler.transform(valid[variates])\n",
    "\n",
    "\t\telectricity_dict = {\n",
    "\t\t\t\t\t\t\"train\" : train,\n",
    "\t\t\t\t\t\t\"test\" : test,\n",
    "\t\t\t\t\t\t\"valid\" : valid,\n",
    "\t\t\t\t\t\t\"scaler\" : electricity_scaler\n",
    "\t\t\t\t\t\t}\n",
    "\telse:\n",
    "\t\telectricity_dict = {\n",
    "\t\t\t\t\t\t\"train\" : train,\n",
    "\t\t\t\t\t\t\"test\" : test,\n",
    "\t\t\t\t\t\t\"valid\" : valid,\n",
    "\t\t\t\t\t\t}\n",
    "\treturn electricity_dict\n",
    "\n",
    "def df_to_tensor(df):\n",
    "\tdf.sort_values(by=['id', 'time_idx'], inplace=True)\n",
    "\n",
    "\t# Initialize a list to store tensors\n",
    "\ttensors = []\n",
    "\n",
    "\t# Unique IDs\n",
    "\tunique_ids = df['id'].unique()\n",
    "\n",
    "\t# For each 'id', create a tensor and add it to the list\n",
    "\tfor unique_id in unique_ids:\n",
    "\t\t# Filter rows by 'id'\n",
    "\t\tdf_id = df[df['id'] == unique_id]    \n",
    "\t\t# Select relevant columns and convert to tensor\n",
    "\t\ttensor = torch.tensor(df_id[['power_usage', 'hour', 'day', 'day_of_week', 'month']].values, dtype=torch.float32)\n",
    "\t\t# Add tensor to list\n",
    "\t\ttensors.append(tensor)\n",
    "\n",
    "\t# Concatenate all tensors along a new dimension\n",
    "\tresult_tensor = torch.stack(tensors)\n",
    "\treturn result_tensor\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/electricity/electricity_small.csv')\n",
    "electricity = split_data(df, use_scaler=True)\n",
    "\n",
    "train_tensor = df_to_tensor(electricity[\"train\"])\n",
    "valid_tensor = df_to_tensor(electricity[\"valid\"])\n",
    "test_tensor = df_to_tensor(electricity[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datalaoder n_batches:\n",
      "Train: 80\n",
      "Valid: 376\n",
      "Test: 2111\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# small multi batch dataset for trial\n",
    "train_tensor_small = train_tensor[:1, 0:, :] # 10 ids and 1000 timesteps -> 10 * (1000/144) = 60 timeseries\n",
    "valid_tensor_small = valid_tensor[:20, 0:, :]\n",
    "test_tensor_small = test_tensor[:, 0:, :]\n",
    "\n",
    "\n",
    "valid_tensor_id_1 = valid_tensor[:1, 0:, :]\n",
    "valid_tensor_id_2 =valid_tensor[2:3, 0:, :]\n",
    "\n",
    "# define input and output size for dataset creation and model config\n",
    "window_size = 132\n",
    "pred_length = 12\n",
    "\n",
    "# create dataset with mutiple input timeseries per id accodring to window_length/pred_length\n",
    "train_sliding_window = SlidingWindowTimeSeriesDataset(train_tensor_small, window_size, pred_length)\n",
    "valid_sliding_window = SlidingWindowTimeSeriesDataset(valid_tensor_small, window_size, pred_length)\n",
    "test_sliding_window = SlidingWindowTimeSeriesDataset(test_tensor_small, window_size, pred_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_sliding_window, batch_size=64, shuffle=True, num_workers=5)\n",
    "valid_dataloader = DataLoader(valid_sliding_window, batch_size=32, shuffle=False)\n",
    "test_datalaoder = DataLoader(test_sliding_window, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Datalaoder n_batches:\\nTrain: {len(train_dataloader)}\\nValid: {len(valid_dataloader)}\\nTest: {len(test_datalaoder)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# defining all needed instances\n",
    "config = ModelConfig(lookback_len=window_size, pred_length=pred_length)\n",
    "model = iTransformer(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "writer = SummaryWriter(log_dir='logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 80/80 [00:19<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.17024583357851952\n",
      "Checkpointing succesfull after epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 80/80 [00:19<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.03198554607806727\n",
      "Checkpointing succesfull after epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 80/80 [00:19<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0240678658883553\n",
      "Checkpointing succesfull after epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "\n",
    "def train_one_epoch(epoch, model, device, train_dataloader, optimizer, scheduler, writer):\n",
    "\tglobal_step = 0\n",
    "\tmodel.train()\n",
    "\ttotal_loss = 0\n",
    "\tfor input, target_covariates in tqdm(train_dataloader, desc=f\"Epoch: {epoch}\"):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tinput = input.to(device)\n",
    "\t\toutput_covariates = model(input)\n",
    "\n",
    "\t\t# this can be used for target specific fine-tuning\n",
    "\t\toutput = output_covariates[12][:,:,0]\n",
    "\t\ttarget = target_covariates[:,:,0]\n",
    "\n",
    "\t\tloss = torch.nn.MSELoss()\n",
    "\t\tloss = loss(output_covariates[12], target_covariates)  # compute loss on all variates\n",
    "\t\t#computed_loss = loss(output, target)  # compute loss on target variate\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\twriter.add_scalar('Loss/train', loss, global_step)\n",
    "\t\twriter.add_scalar('LR/train', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "\t\tglobal_step+=1\n",
    "\n",
    "\tprint(f'Epoch {epoch}, Loss: {total_loss / len(train_dataloader)}')\n",
    "\n",
    "\tscheduler.step()\n",
    "\n",
    "\twriter.close()\n",
    "\tcreate_checkpoint(model, optimizer, scheduler, epoch, loss, global_step, \"first_20_ids_full\")\n",
    "\n",
    "for epoch in range(1, epoch + 1):\n",
    "\ttrain_one_epoch(epoch, model, device, train_dataloader, optimizer, scheduler, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_percentiles(predictions, actuals, percentile):\n",
    "\t# take predictions, actuals in correct order and return percentiles\n",
    "\t# return p10, p50, p90\n",
    "\t\n",
    "\terrors = np.abs(predictions - actuals)\n",
    "\n",
    "\t# Sort the errors\n",
    "\tsorted_errors = np.sort(errors)\n",
    "\n",
    "# Calculate p10, P50 and P90\n",
    "\tp_percentile = np.percentile(sorted_errors, percentile)\n",
    "\treturn p_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tensor_id_1 = valid_tensor[:1, 0:, :]\n",
    "valid_tensor_id_2 =valid_tensor[2:3, 0:, :]\n",
    "\n",
    "\n",
    "\n",
    "window_size = 132\n",
    "pred_length = 12\n",
    "\n",
    "# create dataset with mutiple input timeseries per id accodring to window_length/pred_length\n",
    "valid_sliding_window_id1 = SlidingWindowTimeSeriesDataset(valid_tensor_id_1, window_size, pred_length)\n",
    "valid_sliding_window_id2 = SlidingWindowTimeSeriesDataset(valid_tensor_id_2, window_size, pred_length)\n",
    "\n",
    "valid_dataloader_id1 = DataLoader(valid_sliding_window_id1, batch_size=32, shuffle=False)\n",
    "valid_dataloader_id2 = DataLoader(valid_sliding_window_id2, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating:  21%|██        | 4/19 [00:00<00:00, 17.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 19/19 [00:00<00:00, 21.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.04394805421562571\n",
      "P10: 0.0032778435558276746\n",
      "\tP50: 0.026434942344693763\n",
      "P90: 0.4216238235172473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_function(valid_dataloader_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating:  11%|█         | 2/19 [00:00<00:01, 13.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 19/19 [00:01<00:00, 15.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.043910691220509376\n",
      "P10: 0.0032440845952614364\n",
      "\tP50: 0.02638869992408313\n",
      "P90: 0.42143149250432066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_function(valid_dataloader_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(valid_dataloader):\n",
    "\t# loading saved model\n",
    "#\tmodel = iTransformer(config)\n",
    "\n",
    "\tcheckpoint = torch.load(\"model_name_first_20_ids_full_epoch_3.pt\")\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\tstart_epoch = checkpoint['epoch'] + 1  # Start training from \n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\tscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\twriter = SummaryWriter(log_dir='logs')\n",
    "\tglobal_step = checkpoint['global_step_writer']\n",
    "\n",
    "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\tglobal_step_eval = 1\n",
    "\ttotal_p10_eval = 0\n",
    "\ttotal_p50_eval = 0\n",
    "\ttotal_p90_eval = 0\n",
    "\n",
    "\t# Example inference code\n",
    "\twith torch.no_grad():\n",
    "\t\ttotal_loss_eval = 0\n",
    "\t\tfor input, target_covariates in tqdm(valid_dataloader, desc=f\"Epoch: Validating\"):\n",
    "\t\t\tinput = input.to(device)\n",
    "\t\t\toutput_covariates = model(input)\n",
    "\n",
    "\t\t\t# this can be used for target specific fine-tuning\n",
    "\t\t\t#output = output_covariates[12][:,:,0]\n",
    "\t\t\t#target = target_covariates[:,:,0]\n",
    "\n",
    "\t\t\tloss = torch.nn.MSELoss()\n",
    "\t\t\tloss = loss(output_covariates[12], target_covariates)  # compute loss on all variates\n",
    "\t\t\t#computed_loss = loss(output, target)  # compute loss on target variate\n",
    "\n",
    "\t\t\ttotal_loss_eval += loss.item()\n",
    "\t\t\ttotal_p10_eval += calc_percentiles(output_covariates[12], target_covariates, 10)\n",
    "\t\t\ttotal_p50_eval += calc_percentiles(output_covariates[12], target_covariates, 50)\n",
    "\t\t\ttotal_p90_eval += calc_percentiles(output_covariates[12], target_covariates, 90)\n",
    "\n",
    "\n",
    "\tprint(f'MSE Loss: {total_loss_eval / len(valid_dataloader)}\\nP10: { total_p10_eval/ len(valid_dataloader)}\\n\\\n",
    "\tP50: {total_p50_eval/ len(valid_dataloader)}\\nP90: { total_p90_eval/ len(valid_dataloader)}')\n",
    "\n",
    "\twriter.add_scalar('MSE/valid', total_loss_eval/ len(valid_dataloader), global_step_eval)\n",
    "\twriter.add_scalar('P10/valid', total_p10_eval/ len(valid_dataloader), global_step_eval)\n",
    "\twriter.add_scalar('P50/valid', total_p50_eval/ len(valid_dataloader), global_step_eval)\n",
    "\twriter.add_scalar('P90/valid', total_p90_eval/ len(valid_dataloader), global_step_eval)\n",
    "\n",
    "\twriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
