{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be executed to run all experiments using the darts library.\n",
    "\n",
    "Training using the Transformer model is unreliable, sometimes returning nan with the same input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/statsforecast/core.py:26: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, NHiTSModel, TransformerModel, TSMixerModel\n",
    "from darts.utils.losses import *\n",
    "from darts.metrics import metrics as darts_metrics\n",
    "from utils import data_handling, helpers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import config\n",
    "import shutil\n",
    "\n",
    "# Constants\n",
    "DEVICE = [1]\n",
    "IN_LEN = 96\n",
    "OUT_LEN = 96\n",
    "LOSS_FN = torch.nn.MSELoss()\n",
    "LAYER_WIDTH = 256\n",
    "NUM_STACKS = 4\n",
    "NUM_BLOCKS = 2\n",
    "NUM_LAYERS = 3\n",
    "COEFFS_DIM = 5\n",
    "DROPOUT = 0.25\n",
    "VERBOSE = True\n",
    "TRAIN_EPOCHS = 15\n",
    "TUNE_EPOCHS = 5\n",
    "four_weeks = -24*7*4\n",
    "LR = 0.005\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define relevant paths\n",
    "metrics_output_path = config.CONFIG_OUTPUT_PATH[\"darts\"] / \"darts_metrics.csv\"\n",
    "model_path = config.CONFIG_MODEL_LOCATION[\"darts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_source_to_target_id_count(source, target):\n",
    "    \"\"\"\n",
    "    If the target dataset has more series, the source dataset is repeated until same number is reached\n",
    "    Input:  -source & target tensor of shape ID x length\n",
    "    Output: - reshaped source & target tensor with same dimensions\n",
    "    \"\"\"\n",
    "    source_id_count = source[\"train\"].shape[1]\n",
    "    target_id_count = target[\"train\"].shape[1]\n",
    "\n",
    "    full_repeats = target_id_count // source_id_count\n",
    "    remainder = target_id_count % source_id_count\n",
    "\n",
    "    repeated_tensor = source[\"train\"].repeat(1, full_repeats)\n",
    "    remainder_tensor = source[\"train\"][:, :remainder]\n",
    "    source_train = torch.cat((repeated_tensor, remainder_tensor), dim=1)\n",
    "    \n",
    "    assert target_id_count == source_train.size(1), f\"Reshaping was incorrect. Target_train = {target_id_count}, source_train = {source_train.size(1)}.\"\n",
    "\n",
    "    repeated_tensor = source[\"validation\"].repeat(1, full_repeats)\n",
    "    remainder_tensor = source[\"validation\"][:, :remainder]\n",
    "    source_validation = torch.cat((repeated_tensor, remainder_tensor), dim=1)\n",
    "    assert target_id_count == source_validation.size(1), f\"Reshaping was incorrect. Target_val = {target_id_count}, source_val = {source_validation.size(1)}.\"\n",
    "\n",
    "    return source_train, source_validation\n",
    "\n",
    "\n",
    "def process_tl_data(source_data, target_data):\n",
    "    \"\"\"\n",
    "    Source and target data are converted to TimeSeries class\n",
    "    Input:  -source & target tensor of shape ID x length\n",
    "    Output: -dict containing all TimeSeries objects\n",
    "    \"\"\"\n",
    "    # either reshape source or target dataset according to which has less IDs\n",
    "    source_ids = source_data[\"train\"].size(1)\n",
    "    target_ids = target_data[\"test\"].size(1)\n",
    "\n",
    "    fine_tune_horizon = -24*7*4\n",
    "    target_test = target_data[\"test\"]\n",
    "    target_fine_tuning = target_data[\"train\"][fine_tune_horizon:,:]\n",
    "\n",
    "    # remove IDs if source is bigger than target or\n",
    "    # repeat IDs if target is bigger than source\n",
    "    if target_ids < source_ids:\n",
    "        source_train = source_data[\"train\"][:,:target_ids]\n",
    "        source_validation = source_data[\"validation\"][:,:target_ids]\n",
    "    else:\n",
    "        source_train, source_validation = extend_source_to_target_id_count(source_data, target_data)\n",
    "\n",
    "    # convert to TimeSeries dataframe\n",
    "    source_train = TimeSeries.from_values(source_train)\n",
    "    source_validation = TimeSeries.from_values(source_validation)\n",
    "    target_test = TimeSeries.from_values(target_test)\n",
    "    target_fine_tuning = TimeSeries.from_values(target_fine_tuning)\n",
    "    target_train = TimeSeries.from_values(target_data[\"train\"])\n",
    "    target_validation = TimeSeries.from_values(target_data[\"validation\"])\n",
    "\n",
    "    tl_dataset = {\n",
    "                    \"source_train\" : source_train,\n",
    "                    \"source_validation\" : source_validation,\n",
    "                    \"target_fine_tuning\" : target_fine_tuning,\n",
    "                    \"target_test\" : target_test,\n",
    "                    \"target_train\" : target_train,\n",
    "                    \"target_validation\" : target_validation\n",
    "                }\n",
    "\n",
    "    return tl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, target_test):\n",
    "    \"\"\"\n",
    "    Evaluates models on target test set\n",
    "    Input:  -trained model\n",
    "            -List of target test sets shaped according to models\n",
    "\n",
    "    Output: Dict{MSE, MAE}\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # check for last input point and create input/target lists of 96 horizons\n",
    "    forecasting_endpoint = int(len(target_test)) - 96*2\n",
    "    window = [target_test[i:i+96] for i in range(0, forecasting_endpoint, 5)]\n",
    "    target = [target_test[i+96:i+96+96] for i in range(0, forecasting_endpoint, 5)]\n",
    "\n",
    "    # predict over dataloader with slidingwindow implementation and 5 time step shifts for each input\n",
    "    predictions = model.predict(n=96, series=window)\n",
    "\n",
    "    mse = darts_metrics.mse(predictions, target)\n",
    "    mae = darts_metrics.mae(predictions, target)\n",
    "\n",
    "    mse = sum(mse) / len(predictions)\n",
    "    mae = sum(mae) / len(predictions)\n",
    "\n",
    "    return {'MSE': mse, 'MAE': mae}\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_name:str, setup_name:str=\"generic\", checkpointing=True):\n",
    "    \"\"\"\n",
    "    Load and instantiate the specificed model with preset hyperparameters\n",
    "    Input:  -model name\n",
    "            -TL setup name\n",
    "            -checkpointing\n",
    "    Output: -instantiated model\n",
    "    \"\"\"\n",
    "    TRAINER_ARGS = {\"enable_progress_bar\": True, \n",
    "                \"accelerator\": \"gpu\",  \n",
    "                \"devices\" : DEVICE,\n",
    "             }\n",
    "    \n",
    "    saving_name = model_name+\"_\"+setup_name\n",
    "\n",
    "    if model_name == \"Transformer\":\n",
    "        model = TransformerModel(\n",
    "            input_chunk_length=IN_LEN, \n",
    "            output_chunk_length=OUT_LEN,\n",
    "            d_model=LAYER_WIDTH, \n",
    "            nhead=4, \n",
    "            num_encoder_layers=2, \n",
    "            num_decoder_layers=2, \n",
    "            dim_feedforward=LAYER_WIDTH, \n",
    "            dropout=DROPOUT, \n",
    "            activation='relu', \n",
    "            loss_fn=LOSS_FN,\n",
    "            optimizer_kwargs={\"lr\": LR},\n",
    "            use_reversible_instance_norm=True,\n",
    "            pl_trainer_kwargs=TRAINER_ARGS,\n",
    "            model_name=saving_name,\n",
    "            save_checkpoints=checkpointing,\n",
    "            work_dir = model_path,\n",
    "            batch_size=BATCH_SIZE\n",
    "            )\n",
    "        \n",
    "\n",
    "    if model_name == \"TSMixer\":\n",
    "        model = TSMixerModel(\n",
    "        input_chunk_length=IN_LEN, \n",
    "        output_chunk_length=OUT_LEN, \n",
    "        hidden_size=LAYER_WIDTH, \n",
    "        ff_size=LAYER_WIDTH, \n",
    "        num_blocks=NUM_BLOCKS, \n",
    "        activation='ReLU', \n",
    "        dropout=DROPOUT, \n",
    "        loss_fn=LOSS_FN,\n",
    "        norm_type='LayerNorm', \n",
    "        optimizer_kwargs={\"lr\": LR},\n",
    "        use_reversible_instance_norm=True,\n",
    "        pl_trainer_kwargs=TRAINER_ARGS,\n",
    "        model_name= saving_name,\n",
    "        save_checkpoints=checkpointing,\n",
    "        work_dir = model_path,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "        \n",
    "    if model_name == \"NHiTS\":\n",
    "        model = NHiTSModel(\n",
    "        input_chunk_length=IN_LEN,\n",
    "        output_chunk_length=OUT_LEN,\n",
    "        activation='ReLU',\n",
    "        num_stacks=NUM_STACKS,\n",
    "        num_blocks=NUM_BLOCKS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        layer_widths=LAYER_WIDTH,\n",
    "        dropout=DROPOUT,\n",
    "        loss_fn=LOSS_FN,\n",
    "        use_reversible_instance_norm=True,\n",
    "        optimizer_kwargs={\"lr\": LR},\n",
    "        pl_trainer_kwargs=TRAINER_ARGS,\n",
    "        model_name= saving_name,\n",
    "        save_checkpoints=checkpointing,\n",
    "        work_dir = model_path,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def delete_checkpoint(model_name, setup_name):\n",
    "    \"\"\"\n",
    "    Delete checkpoint specified from input\n",
    "    \"\"\"\n",
    "    directory_path = model_path / (model_name + \"_\" + setup_name)\n",
    "    try:\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"File {directory_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file {directory_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electricity dataset\n",
    "electricity_dict = data_handling.format_electricity()\n",
    "\n",
    "for key, value in electricity_dict.items():\n",
    "\t\t\telectricity_dict[key]= data_handling.df_to_tensor(value)\n",
    "\n",
    "# normalize train and use matrics for val and test\n",
    "electricity_dict[\"4_weeks_train\"] = electricity_dict[\"train\"][four_weeks:,:]\n",
    "electricity_dict[\"train\"], train_standardize_dict = helpers.custom_standardizer(electricity_dict[\"train\"])\n",
    "electricity_dict[\"validation\"], _ = helpers.custom_standardizer(electricity_dict[\"validation\"], train_standardize_dict)\n",
    "electricity_dict[\"test\"], _ = helpers.custom_standardizer(electricity_dict[\"test\"], train_standardize_dict)\n",
    "electricity_dict[\"4_weeks_train\"], _ = helpers.custom_standardizer(electricity_dict[\"4_weeks_train\"], train_standardize_dict)\n",
    "\n",
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "bavaria_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "bavaria_dict[\"4_weeks_train\"] = bavaria_dict[\"train\"][four_weeks:,:]\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "gp_dict[\"4_weeks_train\"] = gp_dict[\"train\"][four_weeks:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all dataset combinations\n",
    "tl_setups = {\n",
    "    \"ELD_to_Bavaria\" : (electricity_dict, bavaria_dict), \n",
    "    \"ELD_to_GP2\" : (electricity_dict, gp_dict),\n",
    "    \"Bavaria_to_ELD\" : (bavaria_dict, electricity_dict), \n",
    "    \"Bavaria_to_GP2\" : (bavaria_dict, gp_dict), \n",
    "    \"GP2_to_Bavaria\": (gp_dict, bavaria_dict), \n",
    "    \"GP2_to_ELD\" : (gp_dict, electricity_dict)\n",
    "     }\n",
    "\n",
    "# Define all index columns\n",
    "model_names = [\"NHiTS\",\t\"Transformer\",\t\"TSMixer\"]\n",
    "learning_scenarios = [\"Zero-Shot\", \"four_weeks_tl\", \"full_tl\", \"full_training\", \"four_weeks_training\"]\n",
    "metrics = [\"MSE\", \"MAE\"]\n",
    "\n",
    "# Initialize or load the DataFrame\n",
    "try:\n",
    "    results_df = pd.read_csv(metrics_output_path, index_col=[0, 1, 2])\n",
    "except FileNotFoundError:\n",
    "    index = pd.MultiIndex.from_product([tl_setups.keys(), learning_scenarios, metrics], names=[\"Setup\", \"Learning_scenario\", \"Metric\"])\n",
    "    results_df = pd.DataFrame(columns=model_names, index=index)\n",
    "\n",
    "# Helper functions\n",
    "def update_metrics(setup_name, model_name, learning_scenario, mae, mse):\n",
    "    print(f\"{model_name} {setup_name} {learning_scenario} MSE: {mse}.\")\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n",
    "\n",
    "def is_metric_filled(setup_name, model_name, learning_scenario):\n",
    "    # Check if specific metrics for a model in a setup and fine-tuning scenario are NaN or not\n",
    "    metrics_filled = not results_df.loc[(setup_name, learning_scenario, slice(None)), model_name].isnull().any()\n",
    "    return metrics_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_ELD_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_zero_shot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_full_tl/checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_full_tl/checkpoints/best-epoch=4-val_loss=0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | criterion           | MSELoss             | 0     \n",
      "1 | train_metrics       | MetricCollection    | 0     \n",
      "2 | val_metrics         | MetricCollection    | 0     \n",
      "3 | rin                 | RINorm              | 2.9 K \n",
      "4 | encoder             | Linear              | 372 K \n",
      "5 | positional_encoding | _PositionalEncoding | 0     \n",
      "6 | transformer         | Transformer         | 2.1 M \n",
      "7 | decoder             | Linear              | 35.9 M\n",
      "------------------------------------------------------------\n",
      "38.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.4 M    Total params\n",
      "153.440   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_full_tl/checkpoints/best-epoch=4-val_loss=0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 314/314 [00:16<00:00, 18.67it/s, train_loss=0.766]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=7` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 314/314 [00:19<00:00, 15.84it/s, train_loss=0.766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 18/18 [00:01<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/darts/metrics/metrics.py:1159: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/darts/metrics/metrics.py:242: RuntimeWarning: Mean of empty slice\n",
      "  vals = np.expand_dims(component_reduction(vals, axis=COMP_AX), axis=COMP_AX)\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/darts/metrics/metrics.py:783: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Bavaria_to_GP2 full_tl MSE: nan.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_GP2_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_Bavaria_to_GP2_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_Bavaria_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_Bavaria_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_GP2_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_GP2_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_ELD_zero_shot.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_ELD_short_train.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/TSMixer_GP2_to_ELD_short_train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32214/382587142.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_32214/382587142.py:25: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    }
   ],
   "source": [
    "for setup_name, (source_data, target_data) in tl_setups.items():\n",
    "    # create ts_format\n",
    "    tl_data = process_tl_data(source_data, target_data)\n",
    "    source_train = tl_data[\"source_train\"]\n",
    "    source_val = tl_data[\"source_validation\"]\n",
    "    target_fine_tuning = tl_data[\"target_fine_tuning\"]\n",
    "    target_test = tl_data[\"target_test\"]\n",
    "    target_train = tl_data[\"target_train\"]\n",
    "    target_val = tl_data[\"target_validation\"]\n",
    "\n",
    "    # select model\n",
    "    for model_name in model_names:\n",
    "\n",
    "        trained_model = False\n",
    "\n",
    "        # zero shot\n",
    "        instance_name = setup_name + \"_zero_shot\"\n",
    "        delete_checkpoint(model_name, instance_name)\n",
    "\n",
    "        if not is_metric_filled(setup_name, model_name, \"Zero-Shot\") :\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=True)\n",
    "            model.fit(source_train, val_series=source_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+ instance_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"Zero-Shot\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "            trained_model = True\n",
    "        \n",
    "        # short fine tuning\n",
    "        instance_name = setup_name + \"_zero_shot\" # use zero-shot model because it is not altered\n",
    "        if not is_metric_filled(setup_name, model_name, \"four_weeks_tl\") :\n",
    "            if trained_model == False:\n",
    "                model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=True)\n",
    "                model.fit(source_train, val_series=source_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+instance_name, best=True)\n",
    "            try: \n",
    "                best_model.fit(target_fine_tuning, epochs=best_model.epochs_trained + TUNE_EPOCHS)\n",
    "            except Exception:\n",
    "                tune_epochs = best_model.epochs_trained + TUNE_EPOCHS\n",
    "                best_model.fit(target_fine_tuning, epochs=tune_epochs)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"four_weeks_tl\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "\n",
    "        delete_checkpoint(model_name, instance_name)\n",
    "\n",
    "        # long fine tuning\n",
    "        instance_name = setup_name + \"_full_tl\"\n",
    "        if not is_metric_filled(setup_name, model_name, \"full_tl\") :\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=True)\n",
    "            model.fit(source_train, val_series=source_val, epochs=5, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+instance_name, best=True)\n",
    "            try: \n",
    "                best_model.fit(target_train, epochs=best_model.epochs_trained + TUNE_EPOCHS +2)\n",
    "            except Exception:\n",
    "                tune_epochs = best_model.epochs_trained + TUNE_EPOCHS +2\n",
    "                best_model.fit(target_train, epochs=tune_epochs)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"full_tl\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "\n",
    "\n",
    "        delete_checkpoint(model_name, instance_name)\n",
    "\n",
    "        # short baseline\n",
    "        instance_name = setup_name + \"_short_train\"\n",
    "        if not is_metric_filled(setup_name, model_name, \"four_weeks_training\") :\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=True)\n",
    "            model.fit(target_fine_tuning, val_series=target_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+instance_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"four_weeks_training\", metrics['MAE'], metrics['MSE'])\n",
    "            delete_checkpoint(model_name, setup_name)\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "\n",
    "        delete_checkpoint(model_name, instance_name)\n",
    "\n",
    "        # long baseline\n",
    "        instance_name = setup_name + \"_short_train\"\n",
    "\n",
    "        if not is_metric_filled(setup_name, model_name, \"full_training\") :\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=True)\n",
    "            model.fit(target_train, val_series=target_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=instance_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+instance_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"full_training\", metrics['MAE'], metrics['MSE'])\n",
    "            delete_checkpoint(model_name, setup_name)\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "        delete_checkpoint(model_name, instance_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
