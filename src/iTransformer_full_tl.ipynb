{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succesfull\n"
     ]
    }
   ],
   "source": [
    "# TODO which one?\n",
    "#git clone https://github.com/lucidrains/iTransformer.git\n",
    "#import iTransformer\n",
    "import sys\n",
    "sys.path.append('/vol/fob-vol7/nebenf21/reinbene/bene/MA/iTransformer') \n",
    "from iTransformer import iTransformer\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import config\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from utils import data_handling, helpers, training_functions\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 96\n",
    "PRED_LENGTH = (96)\n",
    "TRAIN_EPOCH = 15\n",
    "TUNE_EPOCH = 5\n",
    "\n",
    "FOUR_WEEKS = -24*7*4\n",
    "print(\"Import succesfull\")\n",
    "\n",
    "\n",
    "metrics_output_path = config.CONFIG_OUTPUT_PATH[\"itransformer\"] / \"itransformer_results_transfer_learning.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning using iTransformer\n",
    "\n",
    "This notebook loads and processes all 3 datasets. The checkpoints from the baseline notebook are used to run the transfer learning experiment for each source-target combination. Because the iTransformers channel number can be changed, we only need 3 trained source model, which makes the training a lot lore efficient than the experiments run using the Darts library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 348])\n",
      "Feature batch shape: torch.Size([32, 96, 59])\n",
      "Feature batch shape: torch.Size([32, 96, 1454])\n"
     ]
    }
   ],
   "source": [
    "# electricity dataset\n",
    "data_dict = data_handling.load_electricity()\n",
    "\n",
    "electricity_dict = {}\n",
    "electricity_dict[\"dataloader_train\"], electricity_dict[\"dataloader_validation\"], electricity_dict[\"dataloader_test\"] = data_handling.convert_data(data_dict, WINDOW_SIZE, PRED_LENGTH)\n",
    "\n",
    "# create a smaller subset of the train dataset\n",
    "electricity_dict[\"4_weeks_train\"] = data_dict[\"train\"][FOUR_WEEKS:,:]\n",
    "electricity_dict[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(electricity_dict[\"4_weeks_train\"] , WINDOW_SIZE, PRED_LENGTH)\n",
    "electricity_dict[\"4_weeks_train\"] = data_handling.DataLoader(electricity_dict[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "bavaria_dict = {}\n",
    "bavaria_dict[\"dataloader_train\"], bavaria_dict[\"dataloader_validation\"], bavaria_dict[\"dataloader_test\"] = data_handling.convert_data(data_dict, WINDOW_SIZE, PRED_LENGTH)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "# create a smaller subset of the train dataset\n",
    "bavaria_dict[\"4_weeks_train\"] = data_dict[\"train\"][FOUR_WEEKS:,:]\n",
    "bavaria_dict[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(bavaria_dict[\"4_weeks_train\"] , WINDOW_SIZE, PRED_LENGTH)\n",
    "bavaria_dict[\"4_weeks_train\"] = data_handling.DataLoader(bavaria_dict[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "# convert to datalaoder\n",
    "gp_dict = {}\n",
    "gp_dict[\"dataloader_train\"], gp_dict[\"dataloader_validation\"], gp_dict[\"dataloader_test\"] = data_handling.convert_data(data_dict, WINDOW_SIZE, PRED_LENGTH)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "# create a smaller subset of the train dataset\n",
    "gp_dict[\"4_weeks_train\"] = data_dict[\"train\"][FOUR_WEEKS:,:]\n",
    "gp_dict[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(gp_dict[\"4_weeks_train\"] , WINDOW_SIZE, PRED_LENGTH)\n",
    "gp_dict[\"4_weeks_train\"] = data_handling.DataLoader(gp_dict[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# merge in dataset dict\n",
    "datasets = {\"ELD\" : electricity_dict,\n",
    "            \"GP2\" : gp_dict,\n",
    "\t\t\t\"Bavaria\" : bavaria_dict\n",
    "            }\n",
    "\n",
    "\n",
    "# define parameters for all models\n",
    "best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.2, \n",
    "                        'num_mem_tokens': 4, 'learning_rate': 0.0005}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_setups = {\n",
    "    \"ELD_to_Bavaria\" : [electricity_dict, bavaria_dict, \"ELD\", \"Bavaria\"], \n",
    "    \"ELD_to_GP2\" : [electricity_dict, gp_dict, \"ELD\", \"GP2\"],\n",
    "    \"Bavaria_to_ELD\" : [bavaria_dict, electricity_dict, \"Bavaria\", \"ELD\"], \n",
    "    \"Bavaria_to_GP2\" : [bavaria_dict, gp_dict, \"Bavaria\", \"GP2\"], \n",
    "    \"GP2_to_Bavaria\": [gp_dict, bavaria_dict, \"GP2\", \"Bavaria\"], \n",
    "    \"GP2_to_ELD\" : [gp_dict, electricity_dict, \"GP2\", \"ELD\"]\n",
    "     }\n",
    "\n",
    "\n",
    "try:\n",
    "    results_df = pd.read_csv(metrics_output_path, index_col=[0, 1, 2])\n",
    "except FileNotFoundError:\n",
    "    metrics = [\"MSE\", \"MAE\"]\n",
    "    learning_scenarios = [\"Zero-Shot\", \"four_weeks_tl\", \"full_tl\", \"full_training\", \"four_weeks_training\"]\n",
    "    index = pd.MultiIndex.from_product([tl_setups.keys(), learning_scenarios, metrics], names=[\"Setup\", \"Learning_scenario\", \"Metric\"])\n",
    "    results_df = pd.DataFrame(columns=[\"iTransformer\"], index=index)\n",
    "\n",
    "# Helper functions\n",
    "def update_metrics(setup_name, model_name, learning_scenario, mae, mse):\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot prediction on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 86.48it/s] \n",
      "/tmp/ipykernel_10716/2943203897.py:21: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_10716/2943203897.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:12<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:06<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:14<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 123.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:13<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tl_setups.items():\n",
    "    source_df = value[0]\n",
    "    target_df = value[1]\n",
    "    source_name = value[2]\n",
    "    target_name = value[3]\n",
    "\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"itransformer\"] / source_name / f\"{source_name}_full_dataset_best_val_loss.pt\"\n",
    "\n",
    "    dataloader = target_df[\"dataloader_test\"]\n",
    "\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    num_variates = inputs.size(2)\n",
    "\n",
    "    model_config = {\n",
    "            'num_variates': num_variates,\n",
    "            'lookback_len': WINDOW_SIZE,\n",
    "            'depth': best_parameters[\"depth\"],\n",
    "            'dim': best_parameters[\"dim\"],\n",
    "            'num_tokens_per_variate': 1,\n",
    "            'pred_length': PRED_LENGTH,\n",
    "            'dim_head': best_parameters[\"dim_head\"],\n",
    "            'heads': best_parameters[\"heads\"],\n",
    "            'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "            'ff_mult': best_parameters[\"ff_mult\"],\n",
    "            'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "            'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "            'use_reversible_instance_norm': True,\n",
    "            'reversible_instance_norm_affine': True,\n",
    "            'flash_attn': True\n",
    "        }\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "    # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "    # learned affine parameters are series specific and need to be relearned for new series\n",
    "    # value are kept at 1 and 0 for stationary normalization\n",
    "    beta = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] \n",
    "    gamma = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"]\n",
    "\n",
    "    # make affine parameter list longer if target series has more channels\n",
    "    if beta.shape[0] < num_variates:\n",
    "        factor = (num_variates // beta.shape[0]) + 1\n",
    "        beta  = torch.cat([beta] *factor)\n",
    "        gamma = torch.cat([gamma] * factor)\n",
    "\n",
    "    # shorten affine parameters to exact number\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = beta[:num_variates]\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = gamma[:num_variates]\n",
    "\n",
    "    model = iTransformer(**model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    metrics = helpers.full_eval(model, dataloader, device)\n",
    "    update_metrics(key, \"iTransformer\", \"Zero-Shot\", metrics[1], metrics[0])\n",
    "\n",
    "results_df.to_csv(metrics_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and predicitons on the test data\n",
    "\n",
    "We fine tune for 5 epochs on different target datasets training sets length. After every epoch the training and validation loss is logged. For the final evaluation the model with the best validation loss is selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ELD as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:00<00:00, 73.31it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:00<00:00, 66.91it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:00<00:00, 77.56it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:00<00:00, 75.21it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:00<00:00, 67.59it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:00<00:00, 69.22it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:00<00:00, 63.17it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:00<00:00, 74.73it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:00<00:00, 61.04it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:00<00:00, 62.88it/s]\n",
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 221.72it/s]\n",
      "/tmp/ipykernel_10716/2943203897.py:21: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_10716/2943203897.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n",
      "Epoch: 1: 100%|██████████| 304/304 [00:04<00:00, 64.61it/s]\n",
      "Epoch: 2: 100%|██████████| 304/304 [00:05<00:00, 56.58it/s]\n",
      "Epoch: 3: 100%|██████████| 304/304 [00:05<00:00, 55.62it/s]\n",
      "Epoch: 4: 100%|██████████| 304/304 [00:05<00:00, 53.06it/s]\n",
      "Epoch: 5: 100%|██████████| 304/304 [00:05<00:00, 59.53it/s]\n",
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 126.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ELD as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:11<00:00,  1.36it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:09<00:00,  1.53it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:05<00:00,  2.88it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:04<00:00,  3.37it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:05<00:00,  2.75it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:07<00:00,  1.91it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:05<00:00,  2.61it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:07<00:00,  1.89it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:23<00:00,  3.73it/s]\n",
      "Epoch: 1: 100%|██████████| 314/314 [02:57<00:00,  1.77it/s]\n",
      "Epoch: 2: 100%|██████████| 314/314 [02:50<00:00,  1.84it/s]\n",
      "Epoch: 3: 100%|██████████| 314/314 [02:50<00:00,  1.84it/s]\n",
      "Epoch: 4: 100%|██████████| 314/314 [02:45<00:00,  1.90it/s]\n",
      "Epoch: 5: 100%|██████████| 314/314 [02:42<00:00,  1.94it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:31<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bavaria as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:03<00:00,  3.88it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:07<00:00,  1.93it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:05<00:00,  2.61it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:03<00:00,  3.96it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:04<00:00,  3.47it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:00<00:00, 21.25it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:00<00:00, 28.42it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:00<00:00, 19.38it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:00<00:00, 21.93it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:00<00:00, 21.09it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:01<00:00, 69.53it/s]\n",
      "Epoch: 1: 100%|██████████| 151/151 [00:17<00:00,  8.77it/s]\n",
      "Epoch: 2: 100%|██████████| 151/151 [00:31<00:00,  4.75it/s]\n",
      "Epoch: 3: 100%|██████████| 151/151 [00:33<00:00,  4.50it/s]\n",
      "Epoch: 4: 100%|██████████| 151/151 [00:23<00:00,  6.36it/s]\n",
      "Epoch: 5: 100%|██████████| 151/151 [00:35<00:00,  4.29it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:03<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bavaria as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:07<00:00,  2.01it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:05<00:00,  2.91it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:08<00:00,  1.78it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:07<00:00,  2.08it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:08<00:00,  1.82it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:07<00:00,  1.88it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:07<00:00,  1.90it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:08<00:00,  1.70it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:08<00:00,  1.75it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:20<00:00,  4.26it/s]\n",
      "Epoch: 1: 100%|██████████| 314/314 [02:49<00:00,  1.85it/s]\n",
      "Epoch: 2: 100%|██████████| 314/314 [02:46<00:00,  1.89it/s]\n",
      "Epoch: 3: 100%|██████████| 314/314 [02:42<00:00,  1.93it/s]\n",
      "Epoch: 4: 100%|██████████| 314/314 [02:32<00:00,  2.06it/s]\n",
      "Epoch: 5: 100%|██████████| 314/314 [03:00<00:00,  1.74it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:22<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GP2 as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:00<00:00, 72.57it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:00<00:00, 72.61it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:00<00:00, 59.68it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:00<00:00, 62.69it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:00<00:00, 55.79it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:00<00:00, 54.79it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:00<00:00, 57.91it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:00<00:00, 65.08it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:00<00:00, 64.46it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:00<00:00, 58.67it/s]\n",
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 135.02it/s]\n",
      "Epoch: 1: 100%|██████████| 304/304 [00:05<00:00, 57.87it/s]\n",
      "Epoch: 2: 100%|██████████| 304/304 [00:04<00:00, 63.19it/s]\n",
      "Epoch: 3: 100%|██████████| 304/304 [00:04<00:00, 62.69it/s]\n",
      "Epoch: 4: 100%|██████████| 304/304 [00:05<00:00, 54.52it/s]\n",
      "Epoch: 5: 100%|██████████| 304/304 [00:05<00:00, 58.79it/s]\n",
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 181.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GP2 as a source dataset.\n",
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:01<00:00, 14.98it/s]\n",
      "Epoch: 2: 100%|██████████| 15/15 [00:00<00:00, 22.44it/s]\n",
      "Epoch: 3: 100%|██████████| 15/15 [00:02<00:00,  6.49it/s]\n",
      "Epoch: 4: 100%|██████████| 15/15 [00:03<00:00,  4.15it/s]\n",
      "Epoch: 5: 100%|██████████| 15/15 [00:03<00:00,  4.40it/s]\n",
      "Epoch: 6: 100%|██████████| 15/15 [00:05<00:00,  2.67it/s]\n",
      "Epoch: 7: 100%|██████████| 15/15 [00:04<00:00,  3.58it/s]\n",
      "Epoch: 8: 100%|██████████| 15/15 [00:02<00:00,  5.86it/s]\n",
      "Epoch: 9: 100%|██████████| 15/15 [00:03<00:00,  4.54it/s]\n",
      "Epoch: 10: 100%|██████████| 15/15 [00:04<00:00,  3.25it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:05<00:00, 14.44it/s]\n",
      "Epoch: 1: 100%|██████████| 151/151 [01:03<00:00,  2.36it/s]\n",
      "Epoch: 2: 100%|██████████| 151/151 [01:38<00:00,  1.53it/s]\n",
      "Epoch: 3: 100%|██████████| 151/151 [00:57<00:00,  2.64it/s]\n",
      "Epoch: 4: 100%|██████████| 151/151 [00:46<00:00,  3.27it/s]\n",
      "Epoch: 5: 100%|██████████| 151/151 [01:10<00:00,  2.13it/s]\n",
      "Epoch: Validating: 100%|██████████| 86/86 [00:18<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def fine_tune(model, dataloader_train, dataloader_validation, device, epoch=5):\n",
    "    \"\"\"\n",
    "    Fine tune function \n",
    "    Input:  -instantiated model\n",
    "            -train & val dataloader\n",
    "            -device\n",
    "            -n_epoch\n",
    "    Output: -trained model\n",
    "            -cuda device           \n",
    "    \"\"\"\n",
    "    # defining all needed instances\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[\"itransformer\"])\n",
    "\n",
    "    # run model training as mentioned in the original paper\n",
    "    _, model = training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, \\\n",
    "                                                        writer, save_model=False, validate=False)\n",
    "    return model, device\n",
    "\n",
    "\n",
    "\n",
    "for key, value in tl_setups.items():\n",
    "    # execute each learning_scenario for each dataset combination\n",
    "\n",
    "    source_df = value[0]\n",
    "    target_df = value[1]\n",
    "    source_name = value[2]\n",
    "    target_name = value[3]\n",
    "    print(f\"Using {source_name} as a source dataset.\")\n",
    "\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"itransformer\"] / source_name / f\"{source_name}_full_dataset_best_val_loss.pt\"\n",
    "\n",
    "\n",
    "    # do test predictions for both target datasets\n",
    "    dataloader_test = target_df[\"dataloader_test\"]\n",
    "    datalaoder_val = target_df[\"dataloader_validation\"]\n",
    "    fine_tune_dataloader = target_df[\"4_weeks_train\"]  \n",
    "    full_fine_tune_dataloader = target_df[\"dataloader_train\"]        \n",
    "      \n",
    "\n",
    "    inputs, _ = next(iter(dataloader_test))\n",
    "    num_variates = inputs.size(2)\n",
    "\n",
    "    model_config = {\n",
    "        'num_variates': num_variates,\n",
    "        'lookback_len': WINDOW_SIZE,\n",
    "        'depth': best_parameters[\"depth\"],\n",
    "        'dim': best_parameters[\"dim\"],\n",
    "        'num_tokens_per_variate': 1,\n",
    "        'pred_length': PRED_LENGTH,\n",
    "        'dim_head': best_parameters[\"dim_head\"],\n",
    "        'heads': best_parameters[\"heads\"],\n",
    "        'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "        'ff_mult': best_parameters[\"ff_mult\"],\n",
    "        'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "        'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "        'use_reversible_instance_norm': True,\n",
    "        'reversible_instance_norm_affine': True,\n",
    "        'flash_attn': True\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "    # (beta) and (gamma) arrays need to be adapted to num_variates\n",
    "    # learned affine parameters are series specific and need to be relearned for new series\n",
    "    # value are kept at 1 and 0 for stationary normalization\n",
    "    beta = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] \n",
    "    gamma = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"]\n",
    "\n",
    "    # make affine parameter list longer if target series has more channels\n",
    "    if beta.shape[0] < num_variates:\n",
    "        factor = (num_variates // beta.shape[0]) + 1\n",
    "        beta  = torch.cat([beta] *factor)\n",
    "        gamma = torch.cat([gamma] * factor)\n",
    "\n",
    "    # shorten affine parameters to exact number\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = beta[:num_variates]\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = gamma[:num_variates]\n",
    "\n",
    "    model = iTransformer(**model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    \n",
    "    # fine tuning on 4 weeks of target data\n",
    "    model, device = fine_tune(model, fine_tune_dataloader, datalaoder_val, device, epoch=TUNE_EPOCH+5)\n",
    "    metrics_fine_tuned = helpers.full_eval(model, dataloader_test, device)\n",
    "    update_metrics(key, \"iTransformer\", \"four_weeks_tl\", metrics_fine_tuned[1], metrics_fine_tuned[0])\n",
    "\n",
    "    # train on full target dataset\n",
    "    checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "    # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "    # learned affine parameters are series specific and need to be relearned for new series\n",
    "    # value are kept at 1 and 0 for stationary normalization\n",
    "    beta = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] \n",
    "    gamma = checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"]\n",
    "\n",
    "    # make affine parameter list longer if target series has more channels\n",
    "    if beta.shape[0] < num_variates:\n",
    "        factor = (num_variates // beta.shape[0]) + 1\n",
    "        beta  = torch.cat([beta] *factor)\n",
    "        gamma = torch.cat([gamma] * factor)\n",
    "\n",
    "    # shorten affine parameters to exact number\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = beta[:num_variates]\n",
    "    checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = gamma[:num_variates]\n",
    "\n",
    "    model = iTransformer(**model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # fne tuning on 4 weeks of target data\n",
    "    model, device = fine_tune(model, full_fine_tune_dataloader, datalaoder_val, device, epoch=TUNE_EPOCH)\n",
    "    metrics_fine_tuned = helpers.full_eval(model, dataloader_test, device)\n",
    "    update_metrics(key, \"iTransformer\", \"full_tl\", metrics_fine_tuned[1], metrics_fine_tuned[0])\n",
    "\n",
    "results_df.to_csv(metrics_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>iTransformer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Setup</th>\n",
       "      <th>Learning_scenario</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">ELD_to_Bavaria</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.195043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.398467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.021556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.009052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">ELD_to_GP2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.866210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.525141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.508224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.380422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.451453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.346905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Bavaria_to_ELD</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>2.735579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.173639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.337294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.405915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.211059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.301230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Bavaria_to_GP2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>2.493282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>1.003410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.604294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.456946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.467799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.363536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">GP2_to_Bavaria</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.232959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.435024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.010998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">GP2_to_ELD</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Zero-Shot</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.450419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.451319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.221454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.300457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_tl</th>\n",
       "      <th>MSE</th>\n",
       "      <td>0.180491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.265729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">four_weeks_training</th>\n",
       "      <th>MSE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           iTransformer\n",
       "Setup          Learning_scenario   Metric              \n",
       "ELD_to_Bavaria Zero-Shot           MSE         0.195043\n",
       "                                   MAE         0.398467\n",
       "               four_weeks_tl       MSE         0.000837\n",
       "                                   MAE         0.021556\n",
       "               full_tl             MSE         0.000255\n",
       "                                   MAE         0.009052\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN\n",
       "ELD_to_GP2     Zero-Shot           MSE         0.866210\n",
       "                                   MAE         0.525141\n",
       "               four_weeks_tl       MSE         0.508224\n",
       "                                   MAE         0.380422\n",
       "               full_tl             MSE         0.451453\n",
       "                                   MAE         0.346905\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN\n",
       "Bavaria_to_ELD Zero-Shot           MSE         2.735579\n",
       "                                   MAE         1.173639\n",
       "               four_weeks_tl       MSE         0.337294\n",
       "                                   MAE         0.405915\n",
       "               full_tl             MSE         0.211059\n",
       "                                   MAE         0.301230\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN\n",
       "Bavaria_to_GP2 Zero-Shot           MSE         2.493282\n",
       "                                   MAE         1.003410\n",
       "               four_weeks_tl       MSE         0.604294\n",
       "                                   MAE         0.456946\n",
       "               full_tl             MSE         0.467799\n",
       "                                   MAE         0.363536\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN\n",
       "GP2_to_Bavaria Zero-Shot           MSE         0.232959\n",
       "                                   MAE         0.435024\n",
       "               four_weeks_tl       MSE         0.000423\n",
       "                                   MAE         0.013992\n",
       "               full_tl             MSE         0.000309\n",
       "                                   MAE         0.010998\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN\n",
       "GP2_to_ELD     Zero-Shot           MSE         0.450419\n",
       "                                   MAE         0.451319\n",
       "               four_weeks_tl       MSE         0.221454\n",
       "                                   MAE         0.300457\n",
       "               full_tl             MSE         0.180491\n",
       "                                   MAE         0.265729\n",
       "               full_training       MSE              NaN\n",
       "                                   MAE              NaN\n",
       "               four_weeks_training MSE              NaN\n",
       "                                   MAE              NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df # Visualise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
