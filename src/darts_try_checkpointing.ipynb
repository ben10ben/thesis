{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be executed to run all experiments using the darts library.\n",
    "\n",
    "Training using the Transformer model is unreliable, sometimes returning nan with the same input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/statsforecast/core.py:26: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, NHiTSModel, TransformerModel, TSMixerModel\n",
    "from darts.utils.losses import *\n",
    "from darts.metrics import metrics as darts_metrics\n",
    "from utils import data_handling, helpers\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import config\n",
    "import copy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import shutil\n",
    "\n",
    "# Constants\n",
    "DEVICE = [1]\n",
    "IN_LEN = 96\n",
    "OUT_LEN = 96\n",
    "LOSS_FN = torch.nn.MSELoss()\n",
    "LAYER_WIDTH = 256\n",
    "NUM_STACKS = 4\n",
    "NUM_BLOCKS = 2\n",
    "NUM_LAYERS = 3\n",
    "COEFFS_DIM = 5\n",
    "DROPOUT = 0.25\n",
    "VERBOSE = True\n",
    "TRAIN_EPOCHS = 15\n",
    "TUNE_EPOCHS = 5\n",
    "four_weeks = -24*7*4\n",
    "LR = 0.005\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "metrics_output_path = config.CONFIG_OUTPUT_PATH[\"darts\"] / \"darts_metrics.csv\"\n",
    "\n",
    "model_path = config.CONFIG_MODEL_LOCATION[\"darts\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_source_to_target_id_count(source, target):\n",
    "    source_id_count = source[\"train\"].shape[1]\n",
    "    target_id_count = target[\"train\"].shape[1]\n",
    "\n",
    "    full_repeats = target_id_count // source_id_count\n",
    "    remainder = target_id_count % source_id_count\n",
    "\n",
    "    repeated_tensor = source[\"train\"].repeat(1, full_repeats)\n",
    "    remainder_tensor = source[\"train\"][:, :remainder]\n",
    "    source_train = torch.cat((repeated_tensor, remainder_tensor), dim=1)\n",
    "    \n",
    "    assert target_id_count == source_train.size(1), f\"Reshaping was incorrect. Target_train = {target_id_count}, source_train = {source_train.size(1)}.\"\n",
    "\n",
    "    repeated_tensor = source[\"validation\"].repeat(1, full_repeats)\n",
    "    remainder_tensor = source[\"validation\"][:, :remainder]\n",
    "    source_validation = torch.cat((repeated_tensor, remainder_tensor), dim=1)\n",
    "    assert target_id_count == source_validation.size(1), f\"Reshaping was incorrect. Target_val = {target_id_count}, source_val = {source_validation.size(1)}.\"\n",
    "\n",
    "    return source_train, source_validation\n",
    "\n",
    "\n",
    "def process_tl_data(source_data, target_data):\n",
    "    # either reshape source or target dataset according to which has less IDs\n",
    "    source_ids = source_data[\"train\"].size(1)\n",
    "    target_ids = target_data[\"test\"].size(1)\n",
    "\n",
    "    fine_tune_horizon = -24*7*4\n",
    "    target_test = target_data[\"test\"]\n",
    "    target_fine_tuning = target_data[\"train\"][fine_tune_horizon:,:]\n",
    "\n",
    "    # remove IDs if source is bigger than target or\n",
    "    # repeat IDs if target is bigger than source\n",
    "    if target_ids < source_ids:\n",
    "        source_train = source_data[\"train\"][:,:target_ids]\n",
    "        source_validation = source_data[\"validation\"][:,:target_ids]\n",
    "    else:\n",
    "        source_train, source_validation = extend_source_to_target_id_count(source_data, target_data)\n",
    "\n",
    "    # convert to TimeSeries dataframe\n",
    "    source_train = TimeSeries.from_values(source_train)\n",
    "    source_validation = TimeSeries.from_values(source_validation)\n",
    "    target_test = TimeSeries.from_values(target_test)\n",
    "    target_fine_tuning = TimeSeries.from_values(target_fine_tuning)\n",
    "    target_train = TimeSeries.from_values(target_data[\"train\"])\n",
    "    target_validation = TimeSeries.from_values(target_data[\"validation\"])\n",
    "\n",
    "    tl_dataset = {\n",
    "                    \"source_train\" : source_train,\n",
    "                    \"source_validation\" : source_validation,\n",
    "                    \"target_fine_tuning\" : target_fine_tuning,\n",
    "                    \"target_test\" : target_test,\n",
    "                    \"target_train\" : target_train,\n",
    "                    \"target_validation\" : target_validation\n",
    "                }\n",
    "\n",
    "    return tl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, target_test):\n",
    "    \"\"\"\n",
    "    Evaluates models on target test set\n",
    "    Input:  -trained model\n",
    "            -List of target test sets shaped according to models\n",
    "\n",
    "    Output: Dict{MSE, MAE}\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # check for last input point and create input/target lists of 96 horizons\n",
    "    forecasting_endpoint = int(len(target_test)) - 96*2\n",
    "    window = [target_test[i:i+96] for i in range(0, forecasting_endpoint, 5)]\n",
    "    target = [target_test[i+96:i+96+96] for i in range(0, forecasting_endpoint, 5)]\n",
    "\n",
    "    # predict over dataloader with slidingwindow implementation and 5 time step shifts for each input\n",
    "    predictions = model.predict(n=96, series=window)\n",
    "\n",
    "    mse = darts_metrics.mse(predictions, target)\n",
    "    mae = darts_metrics.mae(predictions, target)\n",
    "\n",
    "    mse = sum(mse) / len(predictions)\n",
    "    mae = sum(mae) / len(predictions)\n",
    "\n",
    "    return {'MSE': mse, 'MAE': mae}\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_name:str, setup_name:str=\"generic\", checkpointing=True):\n",
    "    TRAINER_ARGS = {\"enable_progress_bar\": True, \n",
    "                \"accelerator\": \"gpu\",  \n",
    "                \"devices\" : DEVICE,\n",
    "             }\n",
    "    \n",
    "    saving_name = model_name+\"_\"+setup_name\n",
    "\n",
    "    print(saving_name)\n",
    "\n",
    "    if model_name == \"Transformer\":\n",
    "        model = TransformerModel(\n",
    "            input_chunk_length=IN_LEN, \n",
    "            output_chunk_length=OUT_LEN,\n",
    "            d_model=LAYER_WIDTH, \n",
    "            nhead=4, \n",
    "            num_encoder_layers=2, \n",
    "            num_decoder_layers=3, \n",
    "            dim_feedforward=LAYER_WIDTH, \n",
    "            dropout=DROPOUT, \n",
    "            activation='relu', \n",
    "            loss_fn=LOSS_FN,\n",
    "            optimizer_kwargs={\"lr\": LR},\n",
    "            use_reversible_instance_norm=True,\n",
    "            pl_trainer_kwargs=TRAINER_ARGS,\n",
    "            model_name=saving_name,\n",
    "            save_checkpoints=checkpointing,\n",
    "            work_dir = model_path,\n",
    "            batch_size=BATCH_SIZE\n",
    "            )\n",
    "        \n",
    "\n",
    "    if model_name == \"TSMixer\":\n",
    "        model = TSMixerModel(\n",
    "        input_chunk_length=IN_LEN, \n",
    "        output_chunk_length=OUT_LEN, \n",
    "        hidden_size=LAYER_WIDTH, \n",
    "        ff_size=LAYER_WIDTH, \n",
    "        num_blocks=NUM_BLOCKS, \n",
    "        activation='ReLU', \n",
    "        dropout=DROPOUT, \n",
    "        loss_fn=LOSS_FN,\n",
    "        norm_type='LayerNorm', \n",
    "        optimizer_kwargs={\"lr\": LR},\n",
    "        use_reversible_instance_norm=True,\n",
    "        pl_trainer_kwargs=TRAINER_ARGS,\n",
    "        model_name= saving_name,\n",
    "        save_checkpoints=checkpointing,\n",
    "        work_dir = model_path,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "        \n",
    "    if model_name == \"NHiTS\":\n",
    "        model = NHiTSModel(\n",
    "        input_chunk_length=IN_LEN,\n",
    "        output_chunk_length=OUT_LEN,\n",
    "        activation='ReLU',\n",
    "        num_stacks=NUM_STACKS,\n",
    "        num_blocks=NUM_BLOCKS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        layer_widths=LAYER_WIDTH,\n",
    "        dropout=DROPOUT,\n",
    "        loss_fn=LOSS_FN,\n",
    "        use_reversible_instance_norm=True,\n",
    "        optimizer_kwargs={\"lr\": LR},\n",
    "        pl_trainer_kwargs=TRAINER_ARGS,\n",
    "        model_name= saving_name,\n",
    "        save_checkpoints=checkpointing,\n",
    "        work_dir = model_path,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def delete_checkpoint(model_name, setup_name):\n",
    "    directory_path = model_path / (model_name + \"_\" + setup_name)\n",
    "    try:\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"File {directory_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file {directory_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use electricity dataset\n",
    "electricity_dict = data_handling.format_electricity()\n",
    "\n",
    "for key, value in electricity_dict.items():\n",
    "\t\t\telectricity_dict[key]= data_handling.df_to_tensor(value)\n",
    "\n",
    "# normalize train and use matrics for val and test\n",
    "electricity_dict[\"4_weeks_train\"] = electricity_dict[\"train\"][four_weeks:,:]\n",
    "electricity_dict[\"train\"], train_standardize_dict = helpers.custom_standardizer(electricity_dict[\"train\"])\n",
    "electricity_dict[\"validation\"], _ = helpers.custom_standardizer(electricity_dict[\"validation\"], train_standardize_dict)\n",
    "electricity_dict[\"test\"], _ = helpers.custom_standardizer(electricity_dict[\"test\"], train_standardize_dict)\n",
    "electricity_dict[\"4_weeks_train\"], _ = helpers.custom_standardizer(electricity_dict[\"4_weeks_train\"], train_standardize_dict)\n",
    "\n",
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "bavaria_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "bavaria_dict[\"4_weeks_train\"] = bavaria_dict[\"train\"][four_weeks:,:]\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "gp_dict[\"4_weeks_train\"] = gp_dict[\"train\"][four_weeks:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_setups = {\n",
    "    \"ELD_to_Bavaria\" : (electricity_dict, bavaria_dict), \n",
    "    \"ELD_to_GP2\" : (electricity_dict, gp_dict),\n",
    "    \"Bavaria_to_ELD\" : (bavaria_dict, electricity_dict), \n",
    "    \"Bavaria_to_GP2\" : (bavaria_dict, gp_dict), \n",
    "    \"GP2_to_Bavaria\": (gp_dict, bavaria_dict), \n",
    "    \"GP2_to_ELD\" : (gp_dict, electricity_dict)\n",
    "     }\n",
    "\n",
    "model_names = [\"NHiTS\",\t\"Transformer\",\t\"TSMixer\"]\n",
    "learning_scenarios = [\"Zero-Shot\", \"four_weeks_tl\", \"full_tl\", \"full_training\", \"four_weeks_training\"]\n",
    "metrics = [\"MSE\", \"MAE\"]\n",
    "\n",
    "# Initialize the DataFrame\n",
    "try:\n",
    "    results_df = pd.read_csv(metrics_output_path, index_col=[0, 1, 2])\n",
    "except FileNotFoundError:\n",
    "    index = pd.MultiIndex.from_product([tl_setups.keys(), learning_scenarios, metrics], names=[\"Setup\", \"Learning_scenario\", \"Metric\"])\n",
    "    results_df = pd.DataFrame(columns=model_names, index=index)\n",
    "\n",
    "# Helper functions\n",
    "def update_metrics(setup_name, model_name, learning_scenario, mae, mse):\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
    "    results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n",
    "\n",
    "def is_metric_filled(setup_name, model_name, learning_scenario):\n",
    "    # Check if specific metrics for a model in a setup and fine-tuning scenario are NaN or not\n",
    "    metrics_filled = not results_df.loc[(setup_name, learning_scenario, slice(None)), model_name].isnull().any()\n",
    "    return metrics_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria.\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/torch/random.py:107: UserWarning: CUDA reports that you have 3 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of GPUs.  If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using.  For example, if you are using CPU only, set CUDA_VISIBLE_DEVICES= or devices=[]; if you are using GPU 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rin           | RINorm           | 118   \n",
      "4 | stacks        | ModuleList       | 13.5 M\n",
      "---------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.148    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 151/151 [00:05<00:00, 30.07it/s, train_loss=1.080, val_loss=0.781]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 151/151 [00:06<00:00, 22.66it/s, train_loss=1.080, val_loss=0.781]\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:00<00:00, 26.35it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints/best-epoch=0-val_loss=0.55.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rin           | RINorm           | 118   \n",
      "4 | stacks        | ModuleList       | 13.5 M\n",
      "---------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.148    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored all states from the checkpoint at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints/best-epoch=0-val_loss=0.55.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16/16 [00:00<00:00, 30.94it/s, train_loss=0.00164]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:383: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 16/16 [00:00<00:00, 21.31it/s, train_loss=0.000355]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 16/16 [00:02<00:00,  6.24it/s, train_loss=0.000355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:00<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints/best-epoch=0-val_loss=0.55.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rin           | RINorm           | 118   \n",
      "4 | stacks        | ModuleList       | 13.5 M\n",
      "---------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.148    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria/checkpoints/best-epoch=0-val_loss=0.55.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 304/304 [00:11<00:00, 26.64it/s, train_loss=0.000224]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 304/304 [00:13<00:00, 23.32it/s, train_loss=0.000224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:01<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria deleted successfully.\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rin           | RINorm           | 118   \n",
      "4 | stacks        | ModuleList       | 13.5 M\n",
      "---------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.148    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 16/16 [00:00<00:00, 18.77it/s, train_loss=9.44e-5, val_loss=0.000355] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 16/16 [00:02<00:00,  6.16it/s, train_loss=9.44e-5, val_loss=0.000355]\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:00<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria deleted successfully.\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rin           | RINorm           | 118   \n",
      "4 | stacks        | ModuleList       | 13.5 M\n",
      "---------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.148    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 304/304 [00:10<00:00, 29.72it/s, train_loss=0.000162, val_loss=0.000283]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 304/304 [00:13<00:00, 23.22it/s, train_loss=0.000162, val_loss=0.000283]\n",
      "NHiTS_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:00<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/NHiTS_ELD_to_Bavaria deleted successfully.\n",
      "Error deleting file /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria.\n",
      "Transformer_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | criterion           | MSELoss             | 0     \n",
      "1 | train_metrics       | MetricCollection    | 0     \n",
      "2 | val_metrics         | MetricCollection    | 0     \n",
      "3 | rin                 | RINorm              | 118   \n",
      "4 | encoder             | Linear              | 15.4 K\n",
      "5 | positional_encoding | _PositionalEncoding | 0     \n",
      "6 | transformer         | Transformer         | 2.8 M \n",
      "7 | decoder             | Linear              | 1.5 M \n",
      "------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.968    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 151/151 [00:07<00:00, 19.49it/s, train_loss=1.300, val_loss=0.781]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 151/151 [00:09<00:00, 16.75it/s, train_loss=1.300, val_loss=0.781]\n",
      "Transformer_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 17/17 [00:00<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72139/4192457664.py:23: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MAE\"), model_name] = mae\n",
      "/tmp/ipykernel_72139/4192457664.py:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  results_df.loc[(setup_name, learning_scenario, \"MSE\"), model_name] = mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_ELD_to_Bavaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/py ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/vol/fob-vol7/nebenf21/reinbene/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria/checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at /vol/fob-vol7/nebenf21/reinbene/bene/MA/outputs/models/darts/Transformer_ELD_to_Bavaria/checkpoints/best-epoch=12-val_loss=0.78.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | criterion           | MSELoss             | 0     \n",
      "1 | train_metrics       | MetricCollection    | 0     \n",
      "2 | val_metrics         | MetricCollection    | 0     \n",
      "3 | rin                 | RINorm              | 118   \n",
      "4 | encoder             | Linear              | 15.4 K\n",
      "5 | positional_encoding | _PositionalEncoding | 0     \n",
      "6 | transformer         | Transformer         | 2.8 M \n",
      "7 | decoder             | Linear              | 1.5 M \n",
      "------------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.968    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You restored a checkpoint with current_epoch=12, but you have set Trainer(max_epochs=5).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(work_dir\u001b[38;5;241m=\u001b[39mmodel_path, model_name\u001b[38;5;241m=\u001b[39mmodel_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msetup_name, best\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model\u001b[38;5;241m.\u001b[39mepochs_trained)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_fine_tuning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_trained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTUNE_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate(best_model, target_test)\n\u001b[1;32m     40\u001b[0m update_metrics(setup_name, model_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfour_weeks_tl\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m], metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/darts/utils/torch.py:103\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    102\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:751\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, num_loader_workers)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    747\u001b[0m     series\u001b[38;5;241m=\u001b[39mseq2series(series),\n\u001b[1;32m    748\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mseq2series(past_covariates),\n\u001b[1;32m    749\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mseq2series(future_covariates),\n\u001b[1;32m    750\u001b[0m )\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/darts/utils/torch.py:103\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    102\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:956\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit_from_dataset\u001b[0;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_from_dataset\u001b[39m(\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m     num_loader_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    913\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchForecastingModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    914\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    Train the model with a specific :class:`darts.utils.data.TrainingDataset` instance.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_loader_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_loader_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/darts/models/forecasting/torch_forecasting_model.py:1101\u001b[0m, in \u001b[0;36mTorchForecastingModel._train\u001b[0;34m(self, trainer, model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_training:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m trainer\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# restore optimizers, etc.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring training state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_training_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:293\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_training_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_precision_plugin_state()\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# restore loops and their progress\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# restore optimizers and schedulers state\u001b[39;00m\n",
      "File \u001b[0;32m~/bene/MA/myenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:351\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_loops\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# crash if max_epochs is lower then the current epoch from the checkpoint\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs\n\u001b[1;32m    350\u001b[0m ):\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou restored a checkpoint with current_epoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcurrent_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but you have set Trainer(max_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m     )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: You restored a checkpoint with current_epoch=12, but you have set Trainer(max_epochs=5)."
     ]
    }
   ],
   "source": [
    "for setup_name, (source_data, target_data) in tl_setups.items():\n",
    "    # create ts_format\n",
    "    tl_data = process_tl_data(source_data, target_data)\n",
    "   \n",
    "    source_train = tl_data[\"source_train\"]\n",
    "    source_val = tl_data[\"source_validation\"]\n",
    "    target_fine_tuning = tl_data[\"target_fine_tuning\"]\n",
    "    target_test = tl_data[\"target_test\"]\n",
    "    target_train = tl_data[\"target_train\"]\n",
    "    target_val = tl_data[\"target_validation\"]\n",
    "\n",
    "    # select model\n",
    "    for model_name in model_names:\n",
    "        delete_checkpoint(model_name, setup_name)\n",
    "\n",
    "        trained_model = False\n",
    "\n",
    "        # short fine tuning\n",
    "        if not is_metric_filled(setup_name, model_name, \"Zero-Shot\") :\n",
    "            # zero shot\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=True)\n",
    "            model.fit(source_train, val_series=source_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+setup_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"Zero-Shot\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "            trained_model = True\n",
    "        \n",
    "        # short fine tuning\n",
    "        if not is_metric_filled(setup_name, model_name, \"four_weeks_tl\") :\n",
    "            if trained_model == False:\n",
    "                model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=True)\n",
    "                model.fit(source_train, val_series=source_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+setup_name, best=True)\n",
    "            print(best_model.epochs_trained)\n",
    "            best_model.fit(target_fine_tuning, epochs=best_model.epochs_trained + TUNE_EPOCHS)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"four_weeks_tl\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "            trained_model = True\n",
    "\n",
    "        # long fine tuning\n",
    "        if not is_metric_filled(setup_name, model_name, \"full_tl\") :\n",
    "            if trained_model == False:\n",
    "                model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=True)\n",
    "                model.fit(source_train, val_series=source_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+setup_name, best=True)\n",
    "            best_model.fit(target_train, epochs=best_model.epochs_trained + TUNE_EPOCHS)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"full_tl\", metrics['MAE'], metrics['MSE'])\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "\n",
    "\n",
    "        delete_checkpoint(model_name, setup_name)\n",
    "\n",
    "        # short baseline\n",
    "        if not is_metric_filled(setup_name, model_name, \"full_training\") :\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=True)\n",
    "            model.fit(target_fine_tuning, val_series=target_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+setup_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"full_training\", metrics['MAE'], metrics['MSE'])\n",
    "            delete_checkpoint(model_name, setup_name)\n",
    "            results_df.to_csv(metrics_output_path)\n",
    "\n",
    "        # long baseline\n",
    "        if not is_metric_filled(setup_name, model_name, \"four_weeks_training\") :\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=True)\n",
    "            model.fit(target_train, val_series=target_val, epochs=TRAIN_EPOCHS, verbose=VERBOSE)\n",
    "            model = load_model(model_name=model_name, setup_name=setup_name, checkpointing=False)\n",
    "            best_model = model.load_from_checkpoint(work_dir=model_path, model_name=model_name+\"_\"+setup_name, best=True)\n",
    "            metrics = evaluate(best_model, target_test)\n",
    "            update_metrics(setup_name, model_name, \"four_weeks_training\", metrics['MAE'], metrics['MSE'])\n",
    "            delete_checkpoint(model_name, setup_name)\n",
    "            results_df.to_csv(metrics_output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
