{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succesfull\n"
     ]
    }
   ],
   "source": [
    "# TODO which one?\n",
    "#git clone https://github.com/lucidrains/iTransformer.git\n",
    "#import iTransformer\n",
    "import sys\n",
    "sys.path.append('/vol/fob-vol7/nebenf21/reinbene/bene/MA/iTransformer') \n",
    "from iTransformer import iTransformer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data_handling, helpers, training_functions\n",
    "import config\n",
    "import pandas as pd\n",
    "\n",
    "window_size = 96\n",
    "pred_length = (96)\n",
    "\n",
    "print(\"Import succesfull\")\n",
    "\n",
    "\n",
    "def create_data_subset(data_dict):\n",
    "    \"\"\"\n",
    "    returns last 2, 4, 6, 8 weeks of dataset\n",
    "    \"\"\"\n",
    "    sum_timesteps = data_dict[\"train\"].size(0)\n",
    "\n",
    "    h_per_week = 7 * 24\n",
    "    weeks_dict = {\"2_weeks\" : 2*h_per_week, \n",
    "                \"4_weeks\" : 4*h_per_week, \n",
    "                \"6_weeks\" : 6*h_per_week, \n",
    "                \"8_weeks\" : 8*h_per_week }\n",
    "\n",
    "\n",
    "    for key, value in weeks_dict.items():\n",
    "        weeks_dict[key] = data_dict[\"train\"][(sum_timesteps-value):,:]     \n",
    "\n",
    "    return weeks_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select dataset for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 348])\n",
      "Feature batch shape: torch.Size([32, 96, 208])\n",
      "Feature batch shape: torch.Size([32, 96, 67])\n"
     ]
    }
   ],
   "source": [
    "# use electricity dataset\n",
    "data_dict = data_handling.load_electricity()\n",
    "\n",
    "electricity = {}\n",
    "electricity[\"dataloader_train\"], electricity[\"dataloader_validation\"], electricity[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# add fine-tuning dataloaders\n",
    "electricity[\"fine_tune\"] = create_data_subset(data_dict)\n",
    "for key, value in electricity[\"fine_tune\"].items():\n",
    "    sliding_window_dataset = data_handling.SlidingWindowTimeSeriesDataset(value, 96, 96)\n",
    "    electricity[\"fine_tune\"][key] = data_handling.DataLoader(sliding_window_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# eu electricity data\n",
    "data_tensor = data_handling.eu_electricity_to_tensor()\n",
    "data_dict, standardize_values =  data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to dataloader\n",
    "europe = {}\n",
    "europe[\"dataloader_train\"], europe[\"dataloader_validation\"], europe[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "europe[\"fine_tune\"] = create_data_subset(data_dict)\n",
    "for key, value in europe[\"fine_tune\"].items():\n",
    "    sliding_window_dataset = data_handling.SlidingWindowTimeSeriesDataset(value, 96, 96)\n",
    "    europe[\"fine_tune\"][key] = data_handling.DataLoader(sliding_window_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "bavaria = {}\n",
    "bavaria[\"dataloader_train\"], bavaria[\"dataloader_validation\"], bavaria[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "bavaria[\"fine_tune\"] = create_data_subset(data_dict)\n",
    "for key, value in bavaria[\"fine_tune\"].items():\n",
    "    sliding_window_dataset = data_handling.SlidingWindowTimeSeriesDataset(value, 96, 96)\n",
    "    bavaria[\"fine_tune\"][key] = data_handling.DataLoader(sliding_window_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "datasets = {\"electricity\" : electricity,\n",
    "\t\t\t\"europe\" : europe,\n",
    "\t\t\t\"bavaria\" : bavaria\n",
    "            }\n",
    "\n",
    "\n",
    "best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.2, \n",
    "                        'num_mem_tokens': 4, 'learning_rate': 0.0005}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot prediction on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating:   0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 267/267 [00:04<00:00, 54.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 71/71 [00:00<00:00, 154.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:02<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 71/71 [00:00<00:00, 158.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:02<00:00, 38.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 267/267 [00:04<00:00, 54.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electricity_to_europe</th>\n",
       "      <th>electricity_to_bavaria</th>\n",
       "      <th>europe_to_electricity</th>\n",
       "      <th>europe_to_bavaria</th>\n",
       "      <th>bavaria_to_electricity</th>\n",
       "      <th>bavaria_to_europe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.119492</td>\n",
       "      <td>0.25068</td>\n",
       "      <td>0.462652</td>\n",
       "      <td>0.148181</td>\n",
       "      <td>4.146668</td>\n",
       "      <td>4.208812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   electricity_to_europe  electricity_to_bavaria  europe_to_electricity  \\\n",
       "0               1.119492                 0.25068               0.462652   \n",
       "\n",
       "   europe_to_bavaria  bavaria_to_electricity  bavaria_to_europe  \n",
       "0           0.148181                4.146668           4.208812  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_metrics = pd.DataFrame()\n",
    "\n",
    "for source, value in datasets.items():\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / source / f\"{source}_best_val_loss.pt\"\n",
    "\n",
    "    target_keys = [data_key for data_key in datasets if data_key != source]\n",
    "\n",
    "    for target in target_keys:\n",
    "        dataloader = datasets[target][\"dataloader_test\"]\n",
    "\n",
    "        inputs, _ = next(iter(dataloader))\n",
    "        num_variates = inputs.size(2)\n",
    "\n",
    "        model_config = {\n",
    "            'num_variates': num_variates,\n",
    "            'lookback_len': window_size,\n",
    "            'depth': best_parameters[\"depth\"],\n",
    "            'dim': best_parameters[\"dim\"],\n",
    "            'num_tokens_per_variate': 1,\n",
    "            'pred_length': pred_length,\n",
    "            'dim_head': best_parameters[\"dim_head\"],\n",
    "            'heads': best_parameters[\"heads\"],\n",
    "            'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "            'ff_mult': best_parameters[\"ff_mult\"],\n",
    "            'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "            'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "            'use_reversible_instance_norm': True,\n",
    "            'reversible_instance_norm_affine': False,\n",
    "            'flash_attn': True\n",
    "        }\n",
    "\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "        checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "        # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "        # learned affine parameters are series specific and need to be relearned for new series\n",
    "        # value are kept at 1 and 0 for stationary normalization\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = torch.zeros(num_variates, 1, dtype=torch.float)\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = torch.ones(num_variates, 1, dtype=torch.float)\n",
    "\n",
    "\n",
    "        model = iTransformer(**model_config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "        metrics_dict = helpers.full_eval(model, dataloader, device)\n",
    "        metrics = metrics_dict[96][\"mse\"].item()\n",
    "        \n",
    "        zero_shot_metrics[f\"{source}_to_{target}\"] = pd.Series(metrics)\n",
    "\n",
    "zero_shot_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning predicitons on the test data\n",
    "\n",
    "We fine tune for 10 epochs on different target datasets training sets length. After every epoch the training and validation loss is logged. For the final evaluation the model with the best validation loss is selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m target_keys:\n\u001b[1;32m     26\u001b[0m     dataloader_test \u001b[38;5;241m=\u001b[39m datasets[target][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     inputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))\n\u001b[1;32m     29\u001b[0m     num_variates \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_variates\u001b[39m\u001b[38;5;124m'\u001b[39m: num_variates,\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlookback_len\u001b[39m\u001b[38;5;124m'\u001b[39m: window_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "def fine_tune(model, dataloader_train, dataloader_validation, device, epoch=1):\n",
    "\n",
    "    # defining all needed instances\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[\"iTransformer_baseline\"])\n",
    "\n",
    "    # run model training as mentioned in the original paper\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer)\n",
    "    return model, device\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fine_tuning_metrics = pd.DataFrame()\n",
    "\n",
    "for source, value in datasets.items():\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / source / f\"{source}_best_val_loss.pt\"\n",
    "\n",
    "    target_keys = [data_key for data_key in datasets if data_key != source]\n",
    "\n",
    "    for target in target_keys:\n",
    "        dataloader_test = datasets[target][\"dataloader_test\"]\n",
    "        datalaoder_val = datasets[target][\"dataloader_validation\"]\n",
    "\n",
    "        inputs, _ = next(iter(dataloader))\n",
    "        num_variates = inputs.size(2)\n",
    "\n",
    "        model_config = {\n",
    "            'num_variates': num_variates,\n",
    "            'lookback_len': window_size,\n",
    "            'depth': best_parameters[\"depth\"],\n",
    "            'dim': best_parameters[\"dim\"],\n",
    "            'num_tokens_per_variate': 1,\n",
    "            'pred_length': pred_length,\n",
    "            'dim_head': best_parameters[\"dim_head\"],\n",
    "            'heads': best_parameters[\"heads\"],\n",
    "            'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "            'ff_mult': best_parameters[\"ff_mult\"],\n",
    "            'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "            'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "            'use_reversible_instance_norm': True,\n",
    "            'reversible_instance_norm_affine': False,\n",
    "            'flash_attn': True\n",
    "        }\n",
    "\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "        checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "        # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "        # learned affine parameters are series specific and need to be relearned for new series\n",
    "        # value are kept at 1 and 0 for stationary normalization\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = torch.zeros(num_variates, 1, dtype=torch.float)\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = torch.ones(num_variates, 1, dtype=torch.float)\n",
    "\n",
    "\n",
    "        model = iTransformer(**model_config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        \n",
    "        # INSERT FINE_TUNING LOOPS\n",
    "        fine_tune_dataloader = datasets[target][\"fine_tune\"][\"2_weeks\"]\n",
    "        \n",
    "        # TODO select small train sub-sets\n",
    "        model, device = fine_tune(model, fine_tune_dataloader, datalaoder_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        metrics_fine_tuned = helpers.full_eval(model, dataloader_test, device)\n",
    "        fine_tuning_metrics_list.append(metrics_fine_tuned[96][\"mse\"].item())\n",
    "\n",
    "        metrics_df_fine_tuned[f\"{key}_split_{num_split}_fine_tuned\"] = metrics_list_fine_tuned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        metrics_dict = helpers.full_eval(model, dataloader, device)\n",
    "        metrics = metrics_dict[96][\"mse\"].item()\n",
    "        \n",
    "        zero_shot_metrics[f\"{source}_to_{target}\"] = pd.Series(metrics)\n",
    "\n",
    "zero_shot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:01<00:00, 55.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{96: {'mse': 0.17870073020458221,\n",
       "  'mae': 0.2555026710033417,\n",
       "  'p10': 0.027185220271348953,\n",
       "  'p50': 0.15552885830402374,\n",
       "  'p90': 0.5825834274291992},\n",
       " 192: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 336: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 720: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we try zero shot\n",
    "\n",
    "dataloader_test = dataloader_test_target\n",
    "\n",
    "metrics = helpers.full_eval(model, dataloader_test, device)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: \"metrics\"}, inplace=True)\n",
    "\n",
    "metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/transfer.csv\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune pretrained model on small subset of training dataset and predict on the same test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 948/948 [00:27<00:00, 34.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE-Loss: 0.09739486943883231, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 131/131 [00:01<00:00, 103.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {96: {'mse': tensor(0.6840, device='cuda:1')}, 192: {'mse': 0}, 336: {'mse': 0}, 720: {'mse': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 267/267 [00:03<00:00, 86.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{96: {'mse': 1.006528615951538,\n",
       "  'mae': 0.5427948236465454,\n",
       "  'p10': 0.0423695333302021,\n",
       "  'p50': 0.2917636036872864,\n",
       "  'p90': 1.3048157691955566},\n",
       " 192: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 336: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 720: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we train on a small subset of the training data for multiple epochs and evaluate again\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "for epoch in range(1, epoch + 1):\n",
    "    training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer)\n",
    "\n",
    "metrics = helpers.full_eval(model, dataloader_test, device)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: key}, inplace=True)\n",
    "\n",
    "#metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{key}_epochs{epoch}_revin.csv\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the pretrained model again and do full fine tuning on the target dataset\n",
    "\n",
    "best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.1, \n",
    "                   'num_mem_tokens': 4, 'learning_rate': 0.0005}\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    'num_variates': data_dict[\"train\"].size(1),\n",
    "    'lookback_len': window_size,\n",
    "    'depth': best_parameters[\"depth\"],\n",
    "    'dim': best_parameters[\"dim\"],\n",
    "    'num_tokens_per_variate': 1,\n",
    "    'pred_length': pred_length,\n",
    "    'dim_head': best_parameters[\"dim_head\"],\n",
    "    'heads': best_parameters[\"heads\"],\n",
    "    'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "    'ff_mult': best_parameters[\"ff_mult\"],\n",
    "    'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "    'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "    'use_reversible_instance_norm': False,\n",
    "    'reversible_instance_norm_affine': False,\n",
    "    'flash_attn': True\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_path = config.CONFIG_MODEL_LOCATION[normalization_strategie]\n",
    "checkpoint = torch.load(model_path)\n",
    "model = iTransformer(**model_config).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# defining all needed instances\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_parameters[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[normalization_strategie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with the full bavaria dataset and prediciton on the same test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "dataloader_train, dataloader_validation, dataloader_test = data_handling.convert_data(data_dict, window_size, pred_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we train on a small subset of the training data for multiple epochs and evaluate again\n",
    "\n",
    "epoch = 10\n",
    "training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer)\n",
    "\n",
    "metrics = training_functions.full_eval(model, dataloader_test)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: key}, inplace=True)\n",
    "\n",
    "metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{key}_epochs{epoch}_revin.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
