{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succesfull\n"
     ]
    }
   ],
   "source": [
    "# TODO which one?\n",
    "#git clone https://github.com/lucidrains/iTransformer.git\n",
    "#import iTransformer\n",
    "import sys\n",
    "sys.path.append('/vol/fob-vol7/nebenf21/reinbene/bene/MA/iTransformer') \n",
    "from iTransformer import iTransformer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data_handling, helpers, training_functions\n",
    "import config\n",
    "import pandas as pd\n",
    "\n",
    "window_size = 96\n",
    "pred_length = (96)\n",
    "\n",
    "four_weeks = -24*7*4\n",
    "print(\"Import succesfull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select dataset for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 348])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 96, 59])\n",
      "Feature batch shape: torch.Size([32, 96, 1454])\n"
     ]
    }
   ],
   "source": [
    "# electricity dataset\n",
    "data_dict = data_handling.load_electricity()\n",
    "\n",
    "electricity = {}\n",
    "electricity[\"dataloader_train\"], electricity[\"dataloader_validation\"], electricity[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# create a smaller subset of the train dataset\n",
    "electricity[\"4_weeks_train\"] = data_dict[\"train\"][four_weeks:,:]\n",
    "electricity[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(electricity[\"4_weeks_train\"] , window_size, pred_length)\n",
    "electricity[\"4_weeks_train\"] = data_handling.DataLoader(electricity[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "bavaria = {}\n",
    "bavaria[\"dataloader_train\"], bavaria[\"dataloader_validation\"], bavaria[\"dataloader_test\"] = data_handling.convert_data(data_dict, window_size, pred_length)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "# create a smaller subset of the train dataset\n",
    "bavaria[\"4_weeks_train\"] = data_dict[\"train\"][four_weeks:,:]\n",
    "bavaria[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(bavaria[\"4_weeks_train\"] , window_size, pred_length)\n",
    "bavaria[\"4_weeks_train\"] = data_handling.DataLoader(bavaria[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# building genome project dataset\n",
    "data_tensor = data_handling.load_genome_project_data()\n",
    "gp_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "# convert to datalaoder\n",
    "genome_project = {}\n",
    "genome_project[\"dataloader_train\"], genome_project[\"dataloader_validation\"], genome_project[\"dataloader_test\"] = data_handling.convert_data(gp_dict, window_size, pred_length)\n",
    "\n",
    "# add fine-tuning datalaoders\n",
    "# create a smaller subset of the train dataset\n",
    "genome_project[\"4_weeks_train\"] = gp_dict[\"train\"][four_weeks:,:]\n",
    "genome_project[\"4_weeks_train\"] = data_handling.SlidingWindowTimeSeriesDataset(genome_project[\"4_weeks_train\"] , window_size, pred_length)\n",
    "genome_project[\"4_weeks_train\"] = data_handling.DataLoader(genome_project[\"4_weeks_train\"] , batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# merge in dataset dict\n",
    "datasets = {\"electricity\" : electricity,\n",
    "            #\"genome_project\" : genome_project,\n",
    "\t\t\t\"bavaria\" : bavaria\n",
    "            }\n",
    "\n",
    "\n",
    "# define parameters for all models\n",
    "best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.2, \n",
    "                        'num_mem_tokens': 4, 'learning_rate': 0.0005}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot prediction on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 83/83 [00:00<00:00, 84.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:27<00:00,  3.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electricity_to_bavaria</th>\n",
       "      <th>bavaria_to_electricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.282732</td>\n",
       "      <td>1.522454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498472</td>\n",
       "      <td>0.971253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   electricity_to_bavaria  bavaria_to_electricity\n",
       "0                0.282732                1.522454\n",
       "1                0.498472                0.971253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_metrics = pd.DataFrame()\n",
    "\n",
    "for source, value in datasets.items():\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / source / f\"{source}_full_dataset_best_val_loss.pt\"\n",
    "\n",
    "    target_keys = [data_key for data_key in datasets if data_key != source]\n",
    "\n",
    "    for target in target_keys:\n",
    "        dataloader = datasets[target][\"dataloader_test\"]\n",
    "\n",
    "        inputs, _ = next(iter(dataloader))\n",
    "        num_variates = inputs.size(2)\n",
    "\n",
    "        model_config = {\n",
    "            'num_variates': num_variates,\n",
    "            'lookback_len': window_size,\n",
    "            'depth': best_parameters[\"depth\"],\n",
    "            'dim': best_parameters[\"dim\"],\n",
    "            'num_tokens_per_variate': 1,\n",
    "            'pred_length': pred_length,\n",
    "            'dim_head': best_parameters[\"dim_head\"],\n",
    "            'heads': best_parameters[\"heads\"],\n",
    "            'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "            'ff_mult': best_parameters[\"ff_mult\"],\n",
    "            'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "            'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "            'use_reversible_instance_norm': True,\n",
    "            'reversible_instance_norm_affine': False,\n",
    "            'flash_attn': True\n",
    "        }\n",
    "\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "        checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "        # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "        # learned affine parameters are series specific and need to be relearned for new series\n",
    "        # value are kept at 1 and 0 for stationary normalization\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = torch.zeros(num_variates, 1, dtype=torch.float)\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = torch.ones(num_variates, 1, dtype=torch.float)\n",
    "\n",
    "\n",
    "        model = iTransformer(**model_config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "        metrics_dict = helpers.full_eval(model, dataloader, device)\n",
    "        metrics = metrics_dict[96][\"mse\"].item(),  metrics_dict[96][\"mae\"].item()\n",
    "        \n",
    "        zero_shot_metrics[f\"{source}_to_{target}\"] = pd.Series(metrics)\n",
    "        zero_shot_metrics.index = [\"mse\", \"mae\"]\n",
    "\n",
    "\n",
    "zero_shot_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning predicitons on the test data\n",
    "\n",
    "We fine tune for 10 epochs on different target datasets training sets length. After every epoch the training and validation loss is logged. For the final evaluation the model with the best validation loss is selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 15/15 [00:00<00:00, 28.06it/s]\n",
      "Epoch: Validating: 100%|██████████| 39/39 [00:00<00:00, 266.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(0.0119, device='cuda:1')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 15/15 [00:00<00:00, 83.16it/s]\n",
      "Epoch: Validating: 100%|██████████| 39/39 [00:00<00:00, 281.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(0.0019, device='cuda:1')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 15/15 [00:00<00:00, 85.46it/s]\n",
      "Epoch: Validating: 100%|██████████| 39/39 [00:00<00:00, 268.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(0.0011, device='cuda:1')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 15/15 [00:00<00:00, 106.14it/s]\n",
      "Epoch: Validating: 100%|██████████| 39/39 [00:00<00:00, 258.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(0.0009, device='cuda:1')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 15/15 [00:00<00:00, 94.26it/s]\n",
      "Epoch: Validating: 100%|██████████| 39/39 [00:00<00:00, 160.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'mse': tensor(0.0007, device='cuda:1')}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# fne tuning on 4 weeks of target data\u001b[39;00m\n\u001b[1;32m     69\u001b[0m fine_tune_dataloader \u001b[38;5;241m=\u001b[39m datasets[target][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4_weeks_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]        \n\u001b[0;32m---> 70\u001b[0m model, device \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatalaoder_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m metrics_fine_tuned \u001b[38;5;241m=\u001b[39m helpers\u001b[38;5;241m.\u001b[39mfull_eval(model, dataloader_test, device)\n\u001b[1;32m     74\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics_fine_tuned[\u001b[38;5;241m96\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), metrics_fine_tuned[\u001b[38;5;241m96\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(model, dataloader_train, dataloader_validation, device, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mCONFIG_LOGS_PATH[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miTransformer_baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# run model training as mentioned in the original paper\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m metrics, model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, device\n",
      "File \u001b[0;32m~/bene/MA/src/utils/training_functions.py:76\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer, checkpoint_path, save_model)\u001b[0m\n\u001b[1;32m     73\u001b[0m \twriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/validation\u001b[39m\u001b[38;5;124m'\u001b[39m, (eval_metrics_dict[\u001b[38;5;241m96\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()), epoch)\n\u001b[1;32m     74\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_metrics_dict, \u001b[43mbest_model\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def fine_tune(model, dataloader_train, dataloader_validation, device, epoch=1):\n",
    "\n",
    "    # defining all needed instances\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[\"iTransformer_baseline\"])\n",
    "\n",
    "    # run model training as mentioned in the original paper\n",
    "    metrics, model = training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer, save_model=False)\n",
    "    return model, device\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fine_tuning_metrics_df = pd.DataFrame()\n",
    "\n",
    "# select a source dataset\n",
    "for source, value in datasets.items():\n",
    "    model_path = config.CONFIG_MODEL_LOCATION[\"iTransformer_baseline\"] / source / f\"{source}_full_dataset_best_val_loss.pt\"\n",
    "\n",
    "    # select both target datasets\n",
    "    target_keys = [data_key for data_key in datasets if data_key != source]\n",
    "\n",
    "    # do test predictions for both target datasets\n",
    "    for target in target_keys:\n",
    "        fine_tuning_metrics_list = []\n",
    "        dataloader_test = datasets[target][\"dataloader_test\"]\n",
    "        datalaoder_val = datasets[target][\"dataloader_validation\"]\n",
    "\n",
    "        inputs, _ = next(iter(dataloader_test))\n",
    "        num_variates = inputs.size(2)\n",
    "\n",
    "        model_config = {\n",
    "            'num_variates': num_variates,\n",
    "            'lookback_len': window_size,\n",
    "            'depth': best_parameters[\"depth\"],\n",
    "            'dim': best_parameters[\"dim\"],\n",
    "            'num_tokens_per_variate': 1,\n",
    "            'pred_length': pred_length,\n",
    "            'dim_head': best_parameters[\"dim_head\"],\n",
    "            'heads': best_parameters[\"heads\"],\n",
    "            'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "            'ff_mult': best_parameters[\"ff_mult\"],\n",
    "            'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "            'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "            'use_reversible_instance_norm': True,\n",
    "            'reversible_instance_norm_affine': True,\n",
    "            'flash_attn': True\n",
    "        }\n",
    "\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "        checkpoint = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "        # values are all set to zero (beta) and one (gamma), array needs to be adapted to num_variates\n",
    "        # learned affine parameters are series specific and need to be relearned for new series\n",
    "        # value are kept at 1 and 0 for stationary normalization\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.beta\"] = torch.zeros(num_variates, 1, dtype=torch.float)\n",
    "        checkpoint[\"model_state_dict\"][\"reversible_instance_norm.gamma\"] = torch.ones(num_variates, 1, dtype=torch.float)\n",
    "\n",
    "\n",
    "        model = iTransformer(**model_config).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        \n",
    "        # fne tuning on 4 weeks of target data\n",
    "        fine_tune_dataloader = datasets[target][\"4_weeks_train\"]        \n",
    "        model, device = fine_tune(model, fine_tune_dataloader, datalaoder_val, device, epoch=5)\n",
    "\n",
    "\n",
    "        metrics_fine_tuned = helpers.full_eval(model, dataloader_test, device)\n",
    "        metrics = metrics_fine_tuned[96][\"mse\"].item(), metrics_fine_tuned[96][\"mae\"].item()\n",
    "\n",
    "        \n",
    "        fine_tuning_metrics_df[f\"{source}_to_{target}\"] = pd.Series(metrics)\n",
    "\n",
    "fine_tuning_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 86/86 [00:01<00:00, 55.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{96: {'mse': 0.17870073020458221,\n",
       "  'mae': 0.2555026710033417,\n",
       "  'p10': 0.027185220271348953,\n",
       "  'p50': 0.15552885830402374,\n",
       "  'p90': 0.5825834274291992},\n",
       " 192: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 336: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 720: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we try zero shot\n",
    "\n",
    "dataloader_test = dataloader_test_target\n",
    "\n",
    "metrics = helpers.full_eval(model, dataloader_test, device)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: \"metrics\"}, inplace=True)\n",
    "\n",
    "metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/transfer.csv\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune pretrained model on small subset of training dataset and predict on the same test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 948/948 [00:27<00:00, 34.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE-Loss: 0.09739486943883231, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 131/131 [00:01<00:00, 103.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {96: {'mse': tensor(0.6840, device='cuda:1')}, 192: {'mse': 0}, 336: {'mse': 0}, 720: {'mse': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: Validating: 100%|██████████| 267/267 [00:03<00:00, 86.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{96: {'mse': 1.006528615951538,\n",
       "  'mae': 0.5427948236465454,\n",
       "  'p10': 0.0423695333302021,\n",
       "  'p50': 0.2917636036872864,\n",
       "  'p90': 1.3048157691955566},\n",
       " 192: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 336: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0},\n",
       " 720: {'mse': 0, 'mae': 0, 'p10': 0, 'p50': 0, 'p90': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we train on a small subset of the training data for multiple epochs and evaluate again\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "for epoch in range(1, epoch + 1):\n",
    "    training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer)\n",
    "\n",
    "metrics = helpers.full_eval(model, dataloader_test, device)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: key}, inplace=True)\n",
    "\n",
    "#metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{key}_epochs{epoch}_revin.csv\")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the pretrained model again and do full fine tuning on the target dataset\n",
    "\n",
    "best_parameters = {'depth': 2, 'dim': 256, 'dim_head': 56, 'heads': 4, 'attn_dropout': 0.2, 'ff_mult': 4, 'ff_dropout': 0.1, \n",
    "                   'num_mem_tokens': 4, 'learning_rate': 0.0005}\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    'num_variates': data_dict[\"train\"].size(1),\n",
    "    'lookback_len': window_size,\n",
    "    'depth': best_parameters[\"depth\"],\n",
    "    'dim': best_parameters[\"dim\"],\n",
    "    'num_tokens_per_variate': 1,\n",
    "    'pred_length': pred_length,\n",
    "    'dim_head': best_parameters[\"dim_head\"],\n",
    "    'heads': best_parameters[\"heads\"],\n",
    "    'attn_dropout': best_parameters[\"attn_dropout\"],\n",
    "    'ff_mult': best_parameters[\"ff_mult\"],\n",
    "    'ff_dropout': best_parameters[\"ff_dropout\"],\n",
    "    'num_mem_tokens': best_parameters[\"num_mem_tokens\"],\n",
    "    'use_reversible_instance_norm': False,\n",
    "    'reversible_instance_norm_affine': False,\n",
    "    'flash_attn': True\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_path = config.CONFIG_MODEL_LOCATION[normalization_strategie]\n",
    "checkpoint = torch.load(model_path)\n",
    "model = iTransformer(**model_config).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# defining all needed instances\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_parameters[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "writer = SummaryWriter(log_dir=config.CONFIG_LOGS_PATH[normalization_strategie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with the full bavaria dataset and prediciton on the same test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bavaria dataset\n",
    "data_tensor = data_handling.load_bavaria_electricity()\n",
    "data_dict, standadizer = data_handling.train_test_split_eu_elec(data_tensor, standardize=True)\n",
    "\n",
    "# convert to datalaoder\n",
    "dataloader_train, dataloader_validation, dataloader_test = data_handling.convert_data(data_dict, window_size, pred_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we train on a small subset of the training data for multiple epochs and evaluate again\n",
    "\n",
    "epoch = 10\n",
    "training_functions.train_one_epoch(epoch, model, device, dataloader_train, dataloader_validation, optimizer, scheduler, writer)\n",
    "\n",
    "metrics = training_functions.full_eval(model, dataloader_test)\n",
    "metrics\n",
    "\n",
    "for eval_metric, value in metrics[96].items():\n",
    "    metrics[96][eval_metric] = value.item()\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics[96], orient='index')\n",
    "metrics_df.rename(columns={0: key}, inplace=True)\n",
    "\n",
    "metrics_df.to_csv(f\"{config.CONFIG_OUTPUT_PATH['iTransformer_baseline']}/metrics_{key}_epochs{epoch}_revin.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
